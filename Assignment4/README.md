# Assignment 4 Deliver Details (st124903_Vorameth)

This assignment will guide you in training a pre-trained model like BERT from scratch, focusing on
leveraging text embeddings to capture semantic similarity. Additionally, we will explore how to adapt the
loss function for tasks like Natural Language Inference (NLI) to enhance the modelâ€™s ability to understand
semantic relationships between texts.

## Task 1: Training BERT from Scratch

In task 1, the training process will be done through the Bidirectional encoder representations from transformers (BERT) which written from scratch from **Prof. Chaklam Silpasuwanchai** ([BERT](https://github.com/chaklam-silpasuwanchai/Python-fo-Natural-Language-Processing/tree/main/Code/02%20-%20DL/04%20-%20Masked%20Language%20Model)). The detailed model is: 
- Number of Encoder of Encoder Layer is 12
- Number of heads in Multi-Head Attention is 12
- Embedding Size is 768
- FeedForward dimension is 768 * 4 
- Dimension of K, Q, and V is is 64
- Number of Segments is 2


For dataset, [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) will be used to train the model which detailed will be:
- Train : 2.12 M Rows
- Validation : 22 K Rows
- Synthetically Generated by GPT-3.5 and GPT-4
- Created on HuggingFace by roneneldan

In this assignment, due to resource limitation, only train with 1 percent (21197 Rows) will be used only for education purpose. 

```python
dataset = load_dataset('roneneldan/TinyStories', split='train[:1%]')
```

Pre-processing in this assignment will be done through following list : 
- Tokenization : Lower Case and Remove Special Characters
- Create Word to ID list
- Create ID to Word list

Moreover, the dataloader is the main focus in BERT model which could help the model to perform according to prefer layers and outputs. Here are some process that will be done for dataloader : 
- input_ids : CLS + token_a + SEP + token_b + SEP = This is done for telling the model with 2 seprate sentence. 
- segment_ids : Identify for the model which token a in sentence a and token b in setence b.
- masked_tokens and masked_pos : Masked tokens for model to predict. 

## Task 2: Sentence Embedding with Sentence BERT

To extend the application of BERT, the Sentence BERT or S-BERT had been introduced which the BERT model will be trained with The Stanford Natural Language Inference (SNLI) Corpus or The Multi-Genre Natural Language Inference (MNLI) corpus for applying classification problem into S-BERT model. Futhuremore, the model will be trained with siamese network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity.

In this assignment, both SNLI and MNLI will be used to train the model. Any preprocess and dataloader will use the coding from the pre-trained model for making sure the input data that should be the same.

    ```python
        snli = datasets.load_dataset('snli')
        mnli = datasets.load_dataset('glue', 'mnli')
        sentences_premise = raw_dataset['train']['premise'] >  batch_a = make_batch()
        sentences_hypothesis = raw_dataset['train']['hypothesis'] >  batch_b = make_batch()
    ```

Then during the training process, the siamese network structures with Classification Objective Function (Softmax Loss) will be focused. 

    ```python
        # predict the last hidden state
        u,_ = model(input_ids_a, segment_ids_a, masked_pos_a)  
        v,_ = model(input_ids_b, segment_ids_b, masked_pos_b)  

        # |u-v| tensor
        uv = torch.sub(u, v)  
        uv_abs = torch.abs(uv) 
        
        # concatenate u, v, |u-v|
        x = torch.cat([u, v, uv_abs], dim=-1) 
        
        # process concatenated tensor through classifier_head
        x = classifier_head(x) 

        # reduce for input into loss funciton
        x = x.mean(dim=1)

        # softmax loss
        loss = criterion(x, label)
    ```

Based on the above softmax loss output, the loss is average with (2 epoches were run due to resource limitation)

## Task 3: Evaluation and Analysis
1. Metrices
    Model Type | SNLI OR MNLI Performance
    --- | --- 
    Our Model | Loss = , Accuracy =  |


2. Analysis
    - Limitations or Challenges 
    - Propose Potential Improvements or Modifications.


## Task 4: Machine Translation - Web Application Development

- This website build based an Attention Mechanism. The following steps show how the input text (English) will be translated (Thai).
    1. Users must input English words that would like to tranlsate into Thai words.
    2. Each word will be feed into the model to encode into a vector and then decode according to probability of targeted word in Thai.
    3. The word will provide possbile of target words based on attention score of input word and other weights. 
    4. The highest probability words will be selected and join into the sentence which will show on the interface.

![website](https://github.com/MrWhiteC/Natural_Language_Understanding_AIT/blob/main/Assignment3/images/a3_website.png)

    