{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Direct Preference Optimization: Your Language Model is Secretly a Reward Model (DPO)](https://arxiv.org/pdf/2305.18290.pdf)\n",
    "\n",
    "### Reference Code \n",
    "- https://huggingface.co/docs/trl/main/en/dpo_trainer\n",
    "- https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the final dataset object should contain these 3 entries if you use the default DPODataCollatorWithPadding data collator. \n",
    "\n",
    "The entries should be named:\n",
    "- prompt\n",
    "- chosen\n",
    "- rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_dataset_dict = {\n",
    "    \"prompt\": [\n",
    "        \"hello\",\n",
    "        \"how are you\",\n",
    "        \"What is your name?\",\n",
    "        \"What is your name?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "    ],\n",
    "    \"chosen\": [\n",
    "        \"hi nice to meet you\",\n",
    "        \"I am fine\",\n",
    "        \"My name is Mary\",\n",
    "        \"My name is Mary\",\n",
    "        \"Python\",\n",
    "        \"Python\",\n",
    "        \"Java\",\n",
    "    ],\n",
    "    \"rejected\": [\n",
    "        \"leave me alone\",\n",
    "        \"I am not fine\",\n",
    "        \"Whats it to you?\",\n",
    "        \"I dont have a name\",\n",
    "        \"Javascript\",\n",
    "        \"C++\",\n",
    "        \"C++\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    HfArgumentParser, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from typing import Dict, Optional\n",
    "from trl import DPOTrainer,DPOConfig\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load a pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"gpt2\"\n",
    "ignore_bias_buffers = False\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "if ignore_bias_buffers:\n",
    "    # torch distributed hack\n",
    "    model._ddp_params_and_buffers_to_ignore = [\n",
    "        name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "    ]\n",
    "\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = load_dataset(\"ProlificAI/social-reasoning-rlhf\",split=\"train[:30%]\")\n",
    "# eval_dataset = load_dataset(\"ProlificAI/social-reasoning-rlhf\",split=\"train[30%:]\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPO trainer expects a model of AutoModelForCausalLM, compared to PPO that expects AutoModelForCausalLMWithValueHead for the value function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Anthropic Helpful-Harmless dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_anthropic_prompt(prompt_and_response):\n",
    "    \"\"\"Extract the anthropic prompt from a prompt and response pair.\"\"\"\n",
    "    search_term = \"Human: \"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "def get_hh(split: str, sanity_check: bool = False, silent: bool = False, cache_dir: str = None) -> Dataset:\n",
    "    \"\"\"Load the Anthropic Helpful-Harmless dataset from Hugging Face and convert it to the necessary format.\n",
    "\n",
    "    The dataset is converted to a dictionary with the following structure:\n",
    "    {\n",
    "        'prompt': List[str],\n",
    "        'chosen': List[str],\n",
    "        'rejected': List[str],\n",
    "    }\n",
    "\n",
    "    Prompts should be structured as follows:\n",
    "      \\n\\nHuman: <prompt>\\n\\nAssistant:\n",
    "    Multiple turns are allowed, but the prompt should always start with \\n\\nHuman: and end with \\n\\nAssistant:.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = load_dataset(\"XueyingJia/hh-rlhf-train-helpful-subset\", split=split, cache_dir=cache_dir)\n",
    "    if sanity_check:\n",
    "        dataset = dataset.select(range(min(len(dataset), 1000)))\n",
    "\n",
    "    def split_prompt_and_responses(sample) -> Dict[str, str]:\n",
    "        prompt = extract_anthropic_prompt(sample[\"prompt\"])\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": sample[\"chosen\"][len(prompt) :],\n",
    "            \"rejected\": sample[\"rejected\"][len(prompt) :],\n",
    "        }\n",
    "\n",
    "    return dataset.map(split_prompt_and_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sanity_check = True\n",
    "train_dataset = get_hh(\"train[30%:]\", sanity_check=sanity_check)\n",
    "eval_dataset = get_hh(\"train[:30%]\", sanity_check=sanity_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. initialize training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "max_length= 512 \n",
    "max_prompt_length = 128 \n",
    "max_target_length =128 \n",
    "label_pad_token_id = 100\n",
    "max_steps = 400\n",
    "# instrumentation\n",
    "sanity_check = True\n",
    "report_to = None\n",
    "gradient_checkpointing = None\n",
    "beta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = DPOConfig(\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    max_steps=max_steps,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=5,  # match results in blog post\n",
    "    eval_steps=500,\n",
    "    output_dir=\"./test_learnrate_1e-1_beta_0.1_grdaient_1\",\n",
    "    optim=\"rmsprop\",\n",
    "    warmup_steps=150,\n",
    "    report_to=report_to,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    # TODO: uncomment that on the next transformers release\n",
    "    # gradient_checkpointing_kwargs=gradient_checkpointing_kwargs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. initialize the DPO trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_length, max_target_length, max_prompt_length, generate_during_eval. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/trl/trainer/dpo_trainer.py:397: UserWarning: You passed `generate_during_eval` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/trl/trainer/dpo_trainer.py:469: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/trl/trainer/dpo_trainer.py:475: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/trl/trainer/dpo_trainer.py:481: UserWarning: You passed `max_target_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    beta=beta,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    max_target_length=max_target_length,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    generate_during_eval=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mworamethr\u001b[0m (\u001b[33mworamethr-asian-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter-st124903/assignment 5_nlp/wandb/run-20250301_064550-v9fp70l2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/woramethr-asian-institute-of-technology/huggingface/runs/v9fp70l2' target=\"_blank\">./test_learnrate_1e-1_beta_0.1_grdaient_1</a></strong> to <a href='https://wandb.ai/woramethr-asian-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/woramethr-asian-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/woramethr-asian-institute-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/woramethr-asian-institute-of-technology/huggingface/runs/v9fp70l2' target=\"_blank\">https://wandb.ai/woramethr-asian-institute-of-technology/huggingface/runs/v9fp70l2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 02:50, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=1.6901643353700637, metrics={'train_runtime': 173.6204, 'train_samples_per_second': 18.431, 'train_steps_per_second': 2.304, 'total_flos': 0.0, 'train_loss': 1.6901643353700637, 'epoch': 3.2})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model(\"dpo_model_rlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd615c6883584d309a1e51a6a7314c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mrwhitec/dpo_model_rlhf/commit/a3fe87d10fe63a29452e70df70b8561fe7aab628', commit_message='Upload model', commit_description='', oid='a3fe87d10fe63a29452e70df70b8561fe7aab628', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mrwhitec/dpo_model_rlhf', endpoint='https://huggingface.co', repo_type='model', repo_id='mrwhitec/dpo_model_rlhf'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"dpo_model_rlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"saved_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"What are signs of being spiritually ascended? Seems many religions, aspire to this.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are signs of being spiritually ascended? Seems many religions, aspire to this.\\n\\nThe first sign of being spiritually ascended is the presence of God. The second sign is the presence of God. The third sign is the presence of God. The fourth sign is the presence of God. The fifth sign is the presence of God'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
