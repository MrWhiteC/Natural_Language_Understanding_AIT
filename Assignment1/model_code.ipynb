{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()\n",
    "news_corpus = brown.sents(categories=['news'])\n",
    "# reference : https://www.nltk.org/howto/corpus.html (brown corpus)\n",
    "\n",
    "# specify windows size which will be used in the random batch funciton for pair the center word with specific numbers of words. \n",
    "windows_size = 4\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus\n",
    "loss_all = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(news_corpus))) \n",
    "vocabs.append('<UNK>')\n",
    "word2index = {v:idx for idx, v in enumerate(vocabs)}\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train data for Skipgram & Skipgram Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def random_batch_skipgrapm(batch_size, news_corpus,windows_size=2): # default windows_size = 2\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "\n",
    "    for doc in news_corpus:\n",
    "        #look from word at the window size number on first and last word\n",
    "        for i in range(windows_size, len(doc)-windows_size):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            #outside words = windows size * 2\n",
    "            outside = []\n",
    "            for j in range(windows_size):\n",
    "                outside.append(word2index[doc[i-j-1]])\n",
    "                outside.append(word2index[doc[i+j+1]]) \n",
    "\n",
    "            for _,each_out in enumerate(outside):\n",
    "                skipgrams.append([center, each_out])\n",
    "\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "            \n",
    "x_skipgrapm, y_skipgramp = random_batch_skipgrapm(2, news_corpus,windows_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 114,\n",
       "         ',': 108,\n",
       "         '.': 89,\n",
       "         'of': 69,\n",
       "         'to': 55,\n",
       "         'and': 55,\n",
       "         'a': 52,\n",
       "         'in': 50,\n",
       "         'for': 30,\n",
       "         'that': 26,\n",
       "         'The': 26,\n",
       "         \"''\": 24,\n",
       "         'was': 24,\n",
       "         'is': 24,\n",
       "         '``': 24,\n",
       "         'on': 22,\n",
       "         'at': 21,\n",
       "         'with': 19,\n",
       "         'be': 19,\n",
       "         'by': 18,\n",
       "         'as': 18,\n",
       "         'he': 17,\n",
       "         'said': 15,\n",
       "         'his': 15,\n",
       "         'will': 15,\n",
       "         'it': 14,\n",
       "         'from': 14,\n",
       "         ';': 13,\n",
       "         'are': 13,\n",
       "         'had': 12,\n",
       "         'has': 12,\n",
       "         '--': 12,\n",
       "         'an': 12,\n",
       "         'not': 11,\n",
       "         'Mrs.': 11,\n",
       "         'have': 11,\n",
       "         'this': 11,\n",
       "         'were': 11,\n",
       "         'who': 11,\n",
       "         'would': 10,\n",
       "         'their': 10,\n",
       "         'which': 10,\n",
       "         'been': 9,\n",
       "         'they': 9,\n",
       "         'He': 9,\n",
       "         'its': 8,\n",
       "         ')': 8,\n",
       "         'last': 8,\n",
       "         'Mr.': 8,\n",
       "         'out': 8,\n",
       "         'more': 8,\n",
       "         'one': 8,\n",
       "         'all': 8,\n",
       "         'but': 8,\n",
       "         'or': 8,\n",
       "         '(': 8,\n",
       "         'up': 8,\n",
       "         'I': 8,\n",
       "         'other': 7,\n",
       "         ':': 7,\n",
       "         'first': 7,\n",
       "         'year': 7,\n",
       "         'than': 7,\n",
       "         'about': 7,\n",
       "         'A': 7,\n",
       "         'new': 7,\n",
       "         'two': 7,\n",
       "         'In': 6,\n",
       "         'home': 6,\n",
       "         'after': 6,\n",
       "         'into': 6,\n",
       "         'It': 6,\n",
       "         'also': 6,\n",
       "         'over': 6,\n",
       "         'there': 6,\n",
       "         'when': 6,\n",
       "         'them': 5,\n",
       "         'her': 5,\n",
       "         'New': 5,\n",
       "         'him': 5,\n",
       "         'any': 5,\n",
       "         'President': 5,\n",
       "         'week': 5,\n",
       "         'time': 5,\n",
       "         'some': 5,\n",
       "         '?': 5,\n",
       "         'no': 5,\n",
       "         'state': 5,\n",
       "         'But': 5,\n",
       "         'only': 5,\n",
       "         'could': 5,\n",
       "         'before': 5,\n",
       "         'years': 5,\n",
       "         'can': 5,\n",
       "         'made': 5,\n",
       "         'three': 5,\n",
       "         'This': 4,\n",
       "         'work': 4,\n",
       "         'House': 4,\n",
       "         'off': 4,\n",
       "         'under': 4,\n",
       "         'school': 4,\n",
       "         'such': 4,\n",
       "         'so': 4,\n",
       "         'back': 4,\n",
       "         'John': 4,\n",
       "         'most': 4,\n",
       "         'we': 4,\n",
       "         'program': 4,\n",
       "         'Kennedy': 4,\n",
       "         'now': 4,\n",
       "         'against': 4,\n",
       "         'American': 4,\n",
       "         'may': 4,\n",
       "         'man': 4,\n",
       "         'if': 4,\n",
       "         'night': 4,\n",
       "         'four': 4,\n",
       "         'here': 4,\n",
       "         'what': 4,\n",
       "         'get': 4,\n",
       "         'members': 4,\n",
       "         'our': 3,\n",
       "         'through': 3,\n",
       "         'did': 3,\n",
       "         'They': 3,\n",
       "         'day': 3,\n",
       "         'system': 3,\n",
       "         'these': 3,\n",
       "         \"'\": 3,\n",
       "         'during': 3,\n",
       "         'yesterday': 3,\n",
       "         'cent': 3,\n",
       "         '1': 3,\n",
       "         'children': 3,\n",
       "         'sales': 3,\n",
       "         'Miss': 3,\n",
       "         'high': 3,\n",
       "         'Texas': 3,\n",
       "         'good': 3,\n",
       "         'United': 3,\n",
       "         'between': 3,\n",
       "         'Mantle': 3,\n",
       "         'those': 3,\n",
       "         'board': 3,\n",
       "         'where': 3,\n",
       "         'Monday': 3,\n",
       "         'million': 3,\n",
       "         'many': 3,\n",
       "         'much': 3,\n",
       "         'jury': 3,\n",
       "         'today': 3,\n",
       "         'car': 3,\n",
       "         'took': 3,\n",
       "         'since': 3,\n",
       "         'administration': 3,\n",
       "         'people': 3,\n",
       "         'even': 3,\n",
       "         'per': 3,\n",
       "         'tax': 3,\n",
       "         'way': 3,\n",
       "         'public': 3,\n",
       "         'told': 3,\n",
       "         'down': 3,\n",
       "         'plan': 3,\n",
       "         'because': 3,\n",
       "         'Washington': 3,\n",
       "         'without': 3,\n",
       "         'got': 3,\n",
       "         'each': 3,\n",
       "         'like': 3,\n",
       "         'bill': 3,\n",
       "         'must': 3,\n",
       "         'you': 3,\n",
       "         'men': 3,\n",
       "         'meeting': 3,\n",
       "         'York': 3,\n",
       "         'both': 3,\n",
       "         'take': 3,\n",
       "         'president': 3,\n",
       "         'State': 3,\n",
       "         'City': 3,\n",
       "         'then': 3,\n",
       "         'do': 3,\n",
       "         'game': 3,\n",
       "         'should': 3,\n",
       "         'government': 3,\n",
       "         'being': 3,\n",
       "         'Dallas': 3,\n",
       "         'U.S.': 3,\n",
       "         'Jr.': 3,\n",
       "         'city': 3,\n",
       "         'Sunday': 3,\n",
       "         'There': 3,\n",
       "         'set': 3,\n",
       "         'service': 2,\n",
       "         'Laos': 2,\n",
       "         '1961': 2,\n",
       "         'White': 2,\n",
       "         'make': 2,\n",
       "         'together': 2,\n",
       "         'county': 2,\n",
       "         'Tuesday': 2,\n",
       "         'general': 2,\n",
       "         'meet': 2,\n",
       "         'club': 2,\n",
       "         'court': 2,\n",
       "         'case': 2,\n",
       "         'Communist': 2,\n",
       "         'too': 2,\n",
       "         'runs': 2,\n",
       "         'party': 2,\n",
       "         'left': 2,\n",
       "         'next': 2,\n",
       "         'help': 2,\n",
       "         'found': 2,\n",
       "         'Committee': 2,\n",
       "         'bonds': 2,\n",
       "         'laws': 2,\n",
       "         'end': 2,\n",
       "         'record': 2,\n",
       "         'market': 2,\n",
       "         'project': 2,\n",
       "         'director': 2,\n",
       "         'annual': 2,\n",
       "         'problem': 2,\n",
       "         'small': 2,\n",
       "         'Co.': 2,\n",
       "         'late': 2,\n",
       "         'Friday': 2,\n",
       "         'month': 2,\n",
       "         'ago': 2,\n",
       "         'May': 2,\n",
       "         'again': 2,\n",
       "         'total': 2,\n",
       "         'top': 2,\n",
       "         'As': 2,\n",
       "         'come': 2,\n",
       "         'go': 2,\n",
       "         'added': 2,\n",
       "         'reported': 2,\n",
       "         'J.': 2,\n",
       "         'several': 2,\n",
       "         '2': 2,\n",
       "         'area': 2,\n",
       "         'hit': 2,\n",
       "         'industry': 2,\n",
       "         'One': 2,\n",
       "         'St.': 2,\n",
       "         'James': 2,\n",
       "         'pay': 2,\n",
       "         'possible': 2,\n",
       "         'Club': 2,\n",
       "         'equipment': 2,\n",
       "         'while': 2,\n",
       "         'Congo': 2,\n",
       "         'Democratic': 2,\n",
       "         'military': 2,\n",
       "         'March': 2,\n",
       "         'S.': 2,\n",
       "         'around': 2,\n",
       "         'great': 2,\n",
       "         'W.': 2,\n",
       "         'taken': 2,\n",
       "         'interest': 2,\n",
       "         'says': 2,\n",
       "         'open': 2,\n",
       "         'still': 2,\n",
       "         'see': 2,\n",
       "         'does': 2,\n",
       "         'past': 2,\n",
       "         'To': 2,\n",
       "         'That': 2,\n",
       "         'run': 2,\n",
       "         'announced': 2,\n",
       "         'center': 2,\n",
       "         'She': 2,\n",
       "         'how': 2,\n",
       "         'she': 2,\n",
       "         'very': 2,\n",
       "         'along': 2,\n",
       "         'received': 2,\n",
       "         'money': 2,\n",
       "         'enough': 2,\n",
       "         'funds': 2,\n",
       "         '&': 2,\n",
       "         'given': 2,\n",
       "         'months': 2,\n",
       "         'staff': 2,\n",
       "         'Player': 2,\n",
       "         'foreign': 2,\n",
       "         'well': 2,\n",
       "         'family': 2,\n",
       "         'best': 2,\n",
       "         'company': 2,\n",
       "         'Saturday': 2,\n",
       "         'Judge': 2,\n",
       "         'later': 2,\n",
       "         'Dr.': 2,\n",
       "         'needed': 2,\n",
       "         'early': 2,\n",
       "         'went': 2,\n",
       "         'States': 2,\n",
       "         '!': 2,\n",
       "         'think': 2,\n",
       "         'countries': 2,\n",
       "         'another': 2,\n",
       "         'Richard': 2,\n",
       "         'right': 2,\n",
       "         'We': 2,\n",
       "         'University': 2,\n",
       "         'just': 2,\n",
       "         'major': 2,\n",
       "         'election': 2,\n",
       "         'season': 2,\n",
       "         'wife': 2,\n",
       "         'never': 2,\n",
       "         'Maris': 2,\n",
       "         'start': 2,\n",
       "         'however': 2,\n",
       "         'big': 2,\n",
       "         'For': 2,\n",
       "         'might': 2,\n",
       "         'schools': 2,\n",
       "         'issue': 2,\n",
       "         'know': 2,\n",
       "         'eight': 2,\n",
       "         'held': 2,\n",
       "         '1960': 2,\n",
       "         'C.': 2,\n",
       "         'E.': 2,\n",
       "         'On': 2,\n",
       "         'Court': 2,\n",
       "         'present': 2,\n",
       "         'until': 2,\n",
       "         'university': 2,\n",
       "         'political': 2,\n",
       "         'County': 2,\n",
       "         'group': 2,\n",
       "         'part': 2,\n",
       "         'committee': 2,\n",
       "         'same': 2,\n",
       "         'Soviet': 2,\n",
       "         'Palmer': 2,\n",
       "         'long': 2,\n",
       "         'North': 2,\n",
       "         'law': 2,\n",
       "         'team': 2,\n",
       "         'far': 2,\n",
       "         '4': 2,\n",
       "         'little': 2,\n",
       "         'federal': 2,\n",
       "         'higher': 2,\n",
       "         'house': 2,\n",
       "         'His': 2,\n",
       "         'came': 2,\n",
       "         'national': 2,\n",
       "         'National': 2,\n",
       "         'ball': 2,\n",
       "         'young': 2,\n",
       "         'ever': 2,\n",
       "         'vote': 2,\n",
       "         'William': 2,\n",
       "         'called': 2,\n",
       "         'need': 2,\n",
       "         'Robert': 2,\n",
       "         'p.m.': 2,\n",
       "         'chairman': 2,\n",
       "         'library': 2,\n",
       "         'began': 2,\n",
       "         'A.': 2,\n",
       "         'And': 2,\n",
       "         'member': 2,\n",
       "         'At': 2,\n",
       "         'session': 2,\n",
       "         'better': 2,\n",
       "         'play': 2,\n",
       "         'put': 2,\n",
       "         'police': 2,\n",
       "         '10': 2,\n",
       "         'use': 2,\n",
       "         'education': 2,\n",
       "         'daughter': 2,\n",
       "         'former': 2,\n",
       "         'my': 2,\n",
       "         'firm': 2,\n",
       "         'result': 2,\n",
       "         'days': 2,\n",
       "         'number': 2,\n",
       "         'show': 2,\n",
       "         'business': 2,\n",
       "         'five': 2,\n",
       "         'few': 2,\n",
       "         'second': 2,\n",
       "         'Republican': 2,\n",
       "         'asked': 2,\n",
       "         'third': 2,\n",
       "         'local': 2,\n",
       "         'When': 2,\n",
       "         'world': 2,\n",
       "         'If': 2,\n",
       "         'H.': 2,\n",
       "         'me': 2,\n",
       "         'give': 2,\n",
       "         'special': 2,\n",
       "         'expected': 2,\n",
       "         'own': 2,\n",
       "         'B.': 2,\n",
       "         'Senate': 2,\n",
       "         'building': 1,\n",
       "         'book': 1,\n",
       "         '1958': 1,\n",
       "         'tried': 1,\n",
       "         'hear': 1,\n",
       "         'earlier': 1,\n",
       "         'especially': 1,\n",
       "         'Park': 1,\n",
       "         'nations': 1,\n",
       "         'Morton': 1,\n",
       "         'charge': 1,\n",
       "         'cars': 1,\n",
       "         'scheduled': 1,\n",
       "         'Barnett': 1,\n",
       "         'N.': 1,\n",
       "         'yards': 1,\n",
       "         'within': 1,\n",
       "         'cases': 1,\n",
       "         'heard': 1,\n",
       "         'failed': 1,\n",
       "         'stand': 1,\n",
       "         'control': 1,\n",
       "         'Nations': 1,\n",
       "         'Board': 1,\n",
       "         'Louis': 1,\n",
       "         'making': 1,\n",
       "         'beat': 1,\n",
       "         'brought': 1,\n",
       "         'Mitchell': 1,\n",
       "         'tell': 1,\n",
       "         'counties': 1,\n",
       "         'seven': 1,\n",
       "         'Ohio': 1,\n",
       "         'couple': 1,\n",
       "         'agreement': 1,\n",
       "         'expansion': 1,\n",
       "         'dinner': 1,\n",
       "         'elected': 1,\n",
       "         'Kansas': 1,\n",
       "         'AP': 1,\n",
       "         'movement': 1,\n",
       "         'art': 1,\n",
       "         'spirit': 1,\n",
       "         'average': 1,\n",
       "         'labor': 1,\n",
       "         'June': 1,\n",
       "         'double': 1,\n",
       "         'struck': 1,\n",
       "         'fees': 1,\n",
       "         'Other': 1,\n",
       "         'include': 1,\n",
       "         'Austin': 1,\n",
       "         'live': 1,\n",
       "         'dropped': 1,\n",
       "         'Stein': 1,\n",
       "         \"President's\": 1,\n",
       "         'private': 1,\n",
       "         '22': 1,\n",
       "         'situation': 1,\n",
       "         'music': 1,\n",
       "         'opportunity': 1,\n",
       "         'upon': 1,\n",
       "         'hope': 1,\n",
       "         'concert': 1,\n",
       "         'working': 1,\n",
       "         'apparently': 1,\n",
       "         'Republicans': 1,\n",
       "         'although': 1,\n",
       "         '14': 1,\n",
       "         'action': 1,\n",
       "         'Department': 1,\n",
       "         'base': 1,\n",
       "         'fall': 1,\n",
       "         'reached': 1,\n",
       "         'Moritz': 1,\n",
       "         'performance': 1,\n",
       "         'agreed': 1,\n",
       "         'himself': 1,\n",
       "         'already': 1,\n",
       "         'served': 1,\n",
       "         'lives': 1,\n",
       "         'son': 1,\n",
       "         'wanted': 1,\n",
       "         'grants': 1,\n",
       "         'women': 1,\n",
       "         'Khrushchev': 1,\n",
       "         'fight': 1,\n",
       "         'San': 1,\n",
       "         'employees': 1,\n",
       "         'test': 1,\n",
       "         'prices': 1,\n",
       "         'race': 1,\n",
       "         'operation': 1,\n",
       "         \"doesn't\": 1,\n",
       "         'More': 1,\n",
       "         'firms': 1,\n",
       "         'seek': 1,\n",
       "         'full': 1,\n",
       "         'April': 1,\n",
       "         'seemed': 1,\n",
       "         'half': 1,\n",
       "         'farm': 1,\n",
       "         'Russia': 1,\n",
       "         '3': 1,\n",
       "         'husband': 1,\n",
       "         'defense': 1,\n",
       "         'leaders': 1,\n",
       "         'driven': 1,\n",
       "         'development': 1,\n",
       "         'Lee': 1,\n",
       "         'move': 1,\n",
       "         'persons': 1,\n",
       "         'Americans': 1,\n",
       "         'won': 1,\n",
       "         'office': 1,\n",
       "         'done': 1,\n",
       "         'clearly': 1,\n",
       "         'civil': 1,\n",
       "         'East': 1,\n",
       "         'Central': 1,\n",
       "         'college': 1,\n",
       "         'least': 1,\n",
       "         'appeared': 1,\n",
       "         'trade': 1,\n",
       "         'receive': 1,\n",
       "         'manager': 1,\n",
       "         'able': 1,\n",
       "         'Denver': 1,\n",
       "         'future': 1,\n",
       "         'Thursday': 1,\n",
       "         'Many': 1,\n",
       "         'reason': 1,\n",
       "         'kept': 1,\n",
       "         'housing': 1,\n",
       "         'So': 1,\n",
       "         '7': 1,\n",
       "         'baseball': 1,\n",
       "         'including': 1,\n",
       "         'Portland': 1,\n",
       "         'Los': 1,\n",
       "         'perhaps': 1,\n",
       "         'necessary': 1,\n",
       "         'Lawrence': 1,\n",
       "         'stage': 1,\n",
       "         'reasons': 1,\n",
       "         'students': 1,\n",
       "         'battle': 1,\n",
       "         'included': 1,\n",
       "         'association': 1,\n",
       "         'Association': 1,\n",
       "         'hand': 1,\n",
       "         'Cuba': 1,\n",
       "         'Her': 1,\n",
       "         \"didn't\": 1,\n",
       "         \"year's\": 1,\n",
       "         'plans': 1,\n",
       "         'previous': 1,\n",
       "         'M.': 1,\n",
       "         'news': 1,\n",
       "         'About': 1,\n",
       "         'Of': 1,\n",
       "         'tournament': 1,\n",
       "         'U.': 1,\n",
       "         'Force': 1,\n",
       "         'Then': 1,\n",
       "         'voters': 1,\n",
       "         'attend': 1,\n",
       "         'either': 1,\n",
       "         'am': 1,\n",
       "         'nation': 1,\n",
       "         'care': 1,\n",
       "         'workers': 1,\n",
       "         'Both': 1,\n",
       "         'final': 1,\n",
       "         'football': 1,\n",
       "         '9': 1,\n",
       "         'developed': 1,\n",
       "         'seem': 1,\n",
       "         'age': 1,\n",
       "         '11': 1,\n",
       "         'junior': 1,\n",
       "         'Christian': 1,\n",
       "         'real': 1,\n",
       "         'Fulton': 1,\n",
       "         '60': 1,\n",
       "         'proposal': 1,\n",
       "         'problems': 1,\n",
       "         'close': 1,\n",
       "         \"don't\": 1,\n",
       "         'name': 1,\n",
       "         'every': 1,\n",
       "         'speed': 1,\n",
       "         '15': 1,\n",
       "         'pitching': 1,\n",
       "         'Last': 1,\n",
       "         'recent': 1,\n",
       "         'Yankees': 1,\n",
       "         'led': 1,\n",
       "         'relations': 1,\n",
       "         'charged': 1,\n",
       "         'payroll': 1,\n",
       "         '21': 1,\n",
       "         'feet': 1,\n",
       "         'Feb.': 1,\n",
       "         'addition': 1,\n",
       "         'calls': 1,\n",
       "         'certain': 1,\n",
       "         'organization': 1,\n",
       "         'others': 1,\n",
       "         'large': 1,\n",
       "         'further': 1,\n",
       "         'British': 1,\n",
       "         'groups': 1,\n",
       "         'trial': 1,\n",
       "         'position': 1,\n",
       "         'revenues': 1,\n",
       "         'hole': 1,\n",
       "         'El': 1,\n",
       "         'bank': 1,\n",
       "         'products': 1,\n",
       "         'D.': 1,\n",
       "         'something': 1,\n",
       "         'Day': 1,\n",
       "         'study': 1,\n",
       "         'Kowalski': 1,\n",
       "         'P.': 1,\n",
       "         'value': 1,\n",
       "         'short': 1,\n",
       "         'personal': 1,\n",
       "         'Council': 1,\n",
       "         'thing': 1,\n",
       "         'above': 1,\n",
       "         'soon': 1,\n",
       "         \"I'm\": 1,\n",
       "         'ruled': 1,\n",
       "         'continued': 1,\n",
       "         'whether': 1,\n",
       "         'campaign': 1,\n",
       "         'mother': 1,\n",
       "         'turned': 1,\n",
       "         'religious': 1,\n",
       "         'Each': 1,\n",
       "         'question': 1,\n",
       "         'lines': 1,\n",
       "         'thought': 1,\n",
       "         'stock': 1,\n",
       "         'drive': 1,\n",
       "         'information': 1,\n",
       "         'hands': 1,\n",
       "         'Miami': 1,\n",
       "         'fine': 1,\n",
       "         'Orleans': 1,\n",
       "         'General': 1,\n",
       "         'indicated': 1,\n",
       "         'really': 1,\n",
       "         'decided': 1,\n",
       "         'minutes': 1,\n",
       "         'Service': 1,\n",
       "         'fire': 1,\n",
       "         'officers': 1,\n",
       "         'effect': 1,\n",
       "         'among': 1,\n",
       "         'conspiracy': 1,\n",
       "         'form': 1,\n",
       "         'Bill': 1,\n",
       "         'aid': 1,\n",
       "         'America': 1,\n",
       "         'hearing': 1,\n",
       "         'Jim': 1,\n",
       "         'became': 1,\n",
       "         'floor': 1,\n",
       "         'current': 1,\n",
       "         'urged': 1,\n",
       "         'loss': 1,\n",
       "         'almost': 1,\n",
       "         'act': 1,\n",
       "         'white': 1,\n",
       "         'measure': 1,\n",
       "         'lost': 1,\n",
       "         'Chicago': 1,\n",
       "         '12': 1,\n",
       "         'Robinson': 1,\n",
       "         'going': 1,\n",
       "         'serious': 1,\n",
       "         'Hill': 1,\n",
       "         'place': 1,\n",
       "         'return': 1,\n",
       "         'league': 1,\n",
       "         'League': 1,\n",
       "         'father': 1,\n",
       "         'estimated': 1,\n",
       "         \"It's\": 1,\n",
       "         'efforts': 1,\n",
       "         'unions': 1,\n",
       "         'Jones': 1,\n",
       "         'period': 1,\n",
       "         \"can't\": 1,\n",
       "         'food': 1,\n",
       "         'key': 1,\n",
       "         'De': 1,\n",
       "         'land': 1,\n",
       "         'health': 1,\n",
       "         'using': 1,\n",
       "         'Sen.': 1,\n",
       "         'sent': 1,\n",
       "         'whose': 1,\n",
       "         'Rev.': 1,\n",
       "         'view': 1,\n",
       "         'admitted': 1,\n",
       "         'according': 1,\n",
       "         'knee': 1,\n",
       "         'Pittsburgh': 1,\n",
       "         \"it's\": 1,\n",
       "         'knew': 1,\n",
       "         'Boston': 1,\n",
       "         'times': 1,\n",
       "         'merely': 1,\n",
       "         'shown': 1,\n",
       "         'wages': 1,\n",
       "         'F.': 1,\n",
       "         'support': 1,\n",
       "         'spent': 1,\n",
       "         'known': 1,\n",
       "         'College': 1,\n",
       "         'Geneva': 1,\n",
       "         'churches': 1,\n",
       "         'pressure': 1,\n",
       "         'candidate': 1,\n",
       "         'Hillsboro': 1,\n",
       "         'Family': 1,\n",
       "         'Hospital': 1,\n",
       "         'considered': 1,\n",
       "         'contract': 1,\n",
       "         'L.': 1,\n",
       "         'Congolese': 1,\n",
       "         'old': 1,\n",
       "         'Frank': 1,\n",
       "         'individual': 1,\n",
       "         'started': 1,\n",
       "         'collection': 1,\n",
       "         'proposed': 1,\n",
       "         'officials': 1,\n",
       "         '6': 1,\n",
       "         'line': 1,\n",
       "         'Senator': 1,\n",
       "         'fund': 1,\n",
       "         'lead': 1,\n",
       "         'executive': 1,\n",
       "         'us': 1,\n",
       "         'Martin': 1,\n",
       "         'modern': 1,\n",
       "         'letters': 1,\n",
       "         'sense': 1,\n",
       "         'feel': 1,\n",
       "         'attack': 1,\n",
       "         'books': 1,\n",
       "         'driving': 1,\n",
       "         'Charles': 1,\n",
       "         'means': 1,\n",
       "         'used': 1,\n",
       "         'nor': 1,\n",
       "         'opinion': 1,\n",
       "         'share': 1,\n",
       "         'Thompson': 1,\n",
       "         'address': 1,\n",
       "         'gin': 1,\n",
       "         'passed': 1,\n",
       "         'shares': 1,\n",
       "         'summer': 1,\n",
       "         'follow': 1,\n",
       "         'Education': 1,\n",
       "         'Sam': 1,\n",
       "         'School': 1,\n",
       "         'hits': 1,\n",
       "         'series': 1,\n",
       "         'coach': 1,\n",
       "         'Joseph': 1,\n",
       "         'decision': 1,\n",
       "         'social': 1,\n",
       "         'word': 1,\n",
       "         'declared': 1,\n",
       "         'tomorrow': 1,\n",
       "         'Howard': 1,\n",
       "         'strike': 1,\n",
       "         'Island': 1,\n",
       "         'passing': 1,\n",
       "         '30': 1,\n",
       "         'generally': 1,\n",
       "         'chief': 1,\n",
       "         'cannot': 1,\n",
       "         'color': 1,\n",
       "         'point': 1,\n",
       "         'Oct.': 1,\n",
       "         'talk': 1,\n",
       "         'bus': 1,\n",
       "         'Hughes': 1,\n",
       "         'With': 1,\n",
       "         'August': 1,\n",
       "         'call': 1,\n",
       "         'free': 1,\n",
       "         'hard': 1,\n",
       "         'neither': 1,\n",
       "         'offered': 1,\n",
       "         'Emory': 1,\n",
       "         'happy': 1,\n",
       "         'headquarters': 1,\n",
       "         'example': 1,\n",
       "         'road': 1,\n",
       "         'governor': 1,\n",
       "         'No': 1,\n",
       "         'front': 1,\n",
       "         'carry': 1,\n",
       "         'Church': 1,\n",
       "         'boy': 1,\n",
       "         'leading': 1,\n",
       "         'paid': 1,\n",
       "         'Wednesday': 1,\n",
       "         'basic': 1,\n",
       "         'across': 1,\n",
       "         'Philadelphia': 1,\n",
       "         '18': 1,\n",
       "         'Government': 1,\n",
       "         'rather': 1,\n",
       "         'companies': 1,\n",
       "         'your': 1,\n",
       "         'once': 1,\n",
       "         'Hotel': 1,\n",
       "         'Mary': 1,\n",
       "         'pass': 1,\n",
       "         'toward': 1,\n",
       "         'increase': 1,\n",
       "         'themselves': 1,\n",
       "         'returned': 1,\n",
       "         'dollars': 1,\n",
       "         'players': 1,\n",
       "         'award': 1,\n",
       "         'town': 1,\n",
       "         'International': 1,\n",
       "         'Premier': 1,\n",
       "         'Arnold': 1,\n",
       "         'possibility': 1,\n",
       "         'cost': 1,\n",
       "         'clear': 1,\n",
       "         'throughout': 1,\n",
       "         'headed': 1,\n",
       "         'parties': 1,\n",
       "         'Georgia': 1,\n",
       "         'shot': 1,\n",
       "         'growth': 1,\n",
       "         'Federal': 1,\n",
       "         'Atlanta': 1,\n",
       "         'Davis': 1,\n",
       "         'evidence': 1,\n",
       "         'After': 1,\n",
       "         'entire': 1,\n",
       "         'nuclear': 1,\n",
       "         'Army': 1,\n",
       "         'build': 1,\n",
       "         'economic': 1,\n",
       "         'income': 1,\n",
       "         'often': 1,\n",
       "         '100': 1,\n",
       "         'country': 1,\n",
       "         '13': 1,\n",
       "         '25': 1,\n",
       "         'games': 1,\n",
       "         '8': 1,\n",
       "         'attorney': 1,\n",
       "         'Catholic': 1,\n",
       "         'fact': 1,\n",
       "         'green': 1,\n",
       "         'District': 1,\n",
       "         'course': 1,\n",
       "         'resolution': 1,\n",
       "         'despite': 1,\n",
       "         'nearly': 1,\n",
       "         'followed': 1,\n",
       "         'R.': 1,\n",
       "         'try': 1,\n",
       "         'Old': 1,\n",
       "         'policy': 1,\n",
       "         'Among': 1,\n",
       "         'textile': 1,\n",
       "         'community': 1,\n",
       "         'single': 1,\n",
       "         'central': 1,\n",
       "         'district': 1,\n",
       "         'concerned': 1,\n",
       "         'recently': 1,\n",
       "         'running': 1,\n",
       "         'Warren': 1,\n",
       "         'July': 1,\n",
       "         'order': 1,\n",
       "         'turn': 1,\n",
       "         'All': 1,\n",
       "         'effective': 1,\n",
       "         'coming': 1,\n",
       "         'secretary': 1,\n",
       "         'immediately': 1,\n",
       "         'department': 1,\n",
       "         'forces': 1,\n",
       "         'Thomas': 1,\n",
       "         'keep': 1,\n",
       "         'weeks': 1,\n",
       "         'statement': 1,\n",
       "         'Their': 1,\n",
       "         'term': 1,\n",
       "         'Democrats': 1,\n",
       "         'read': 1,\n",
       "         'away': 1,\n",
       "         '19': 1,\n",
       "         'afternoon': 1,\n",
       "         'arrested': 1,\n",
       "         '1959': 1,\n",
       "         'amount': 1,\n",
       "         'following': 1,\n",
       "         'hospital': 1,\n",
       "         'condition': 1,\n",
       "         'legislation': 1,\n",
       "         'ready': 1,\n",
       "         'population': 1,\n",
       "         'research': 1,\n",
       "         'verdict': 1,\n",
       "         'salary': 1,\n",
       "         'plane': 1,\n",
       "         'Jan.': 1,\n",
       "         'playing': 1,\n",
       "         'bond': 1,\n",
       "         'These': 1,\n",
       "         'makes': 1,\n",
       "         'leader': 1,\n",
       "         'hour': 1,\n",
       "         'matter': 1,\n",
       "         'opening': 1,\n",
       "         'important': 1,\n",
       "         'whole': 1,\n",
       "         'immediate': 1,\n",
       "         'Moscow': 1,\n",
       "         'event': 1,\n",
       "         'complete': 1,\n",
       "         'An': 1,\n",
       "         'marriage': 1,\n",
       "         'required': 1,\n",
       "         'look': 1,\n",
       "         'council': 1,\n",
       "         'teaching': 1,\n",
       "         'finished': 1,\n",
       "         'water': 1,\n",
       "         'owners': 1,\n",
       "         \"Kennedy's\": 1,\n",
       "         'guests': 1,\n",
       "         'You': 1,\n",
       "         'Houston': 1,\n",
       "         'interested': 1,\n",
       "         'demand': 1,\n",
       "         'judge': 1,\n",
       "         'additional': 1,\n",
       "         'war': 1,\n",
       "         'Street': 1,\n",
       "         'Smith': 1,\n",
       "         'evening': 1,\n",
       "         'Havana': 1,\n",
       "         'Jack': 1,\n",
       "         'particularly': 1,\n",
       "         '5': 1,\n",
       "         'saw': 1,\n",
       "         'Mickey': 1,\n",
       "         'Henry': 1,\n",
       "         'charter': 1,\n",
       "         'field': 1,\n",
       "         'room': 1,\n",
       "         'golf': 1,\n",
       "         'reports': 1,\n",
       "         'faculty': 1,\n",
       "         'Center': 1,\n",
       "         'Corps': 1,\n",
       "         'history': 1,\n",
       "         'Aj': 1,\n",
       "         'medical': 1,\n",
       "         'plus': 1,\n",
       "         'bills': 1,\n",
       "         'caused': 1,\n",
       "         'coal': 1,\n",
       "         'felt': 1,\n",
       "         'anti-trust': 1,\n",
       "         'prevent': 1,\n",
       "         'side': 1,\n",
       "         'score': 1,\n",
       "         'rules': 1,\n",
       "         'independence': 1,\n",
       "         'Two': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 0.001\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(flatten(news_corpus))\n",
    "word_count\n",
    "\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "num_total_words\n",
    "\n",
    "unigram_table = []\n",
    "\n",
    "for v in vocabs:\n",
    "    uw = word_count[v] / num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    unigram_table.extend([v] * uw_alpha)\n",
    "    \n",
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurence Matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "X_i = Counter(flatten(news_corpus))\n",
    "\n",
    "skip_grams = []\n",
    "\n",
    "for doc in news_corpus:\n",
    "    for i in range(windows_size, len(doc)-windows_size):\n",
    "        center = doc[i]\n",
    "        outside = []\n",
    "        for j in range(windows_size):\n",
    "            outside.append(doc[i-j-1])\n",
    "            outside.append(doc[i+j+1]) \n",
    "        for each_out in outside:\n",
    "            skip_grams.append((center, each_out))\n",
    "            \n",
    "X_ik_skipgrams = Counter(skip_grams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "\n",
    "    except:\n",
    "        x_ij = 1\n",
    "        \n",
    "\n",
    "    x_max = 100\n",
    "\n",
    "    alpha = 0.75\n",
    "    \n",
    "\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij / x_max)**alpha\n",
    "\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} \n",
    "weighting_dic = {} \n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):\n",
    "    if X_ik_skipgrams.get(bigram):  \n",
    "        co = X_ik_skipgrams[bigram]\n",
    "        X_ik[bigram] = co + 1 \n",
    "        X_ik[(bigram[1], bigram[0])] = co + 1 \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch_glove(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    random_inputs, random_labels, random_coocs, random_weightings = [], [], [], []\n",
    "    \n",
    "    #convert our skipgrams to id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly choose indexes based on batch size\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False)\n",
    "    \n",
    "    #get the random input and labels\n",
    "    for index in random_index:\n",
    "        random_inputs.append([skip_grams_id[index][0]])\n",
    "        random_labels.append([skip_grams_id[index][1]])\n",
    "        #coocs\n",
    "        pair = skip_grams[index] #e.g., ('banana', 'fruit')\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "    \n",
    "        #weightings\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "x_glove, y_glove, cooc, weighting = random_batch_glove(batch_size, news_corpus, skip_grams, X_ik, weighting_dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(7, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 14392, 14393, 14394],\n",
       "        [    0,     1,     2,  ..., 14392, 14393, 14394]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare all vocabs\n",
    "\n",
    "batch_size = 2\n",
    "voc_size   = len(vocabs)\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(x_skipgrapm)\n",
    "label_tensor = torch.LongTensor(y_skipgramp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    100 | Loss: 9.847179\n",
      "Epoch    200 | Loss: 8.592554\n",
      "Epoch    300 | Loss: 10.407146\n",
      "Epoch    400 | Loss: 9.902851\n",
      "Epoch    500 | Loss: 9.603545\n",
      "Epoch    600 | Loss: 10.437670\n",
      "Epoch    700 | Loss: 9.978737\n",
      "Epoch    800 | Loss: 9.363317\n",
      "Epoch    900 | Loss: 11.957073\n",
      "Epoch   1000 | Loss: 10.606616\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "batch_size = 2\n",
    "emb_size   = 2\n",
    "model_skipgram  = Skipgram(voc_size, emb_size)\n",
    "optimizer  = optim.Adam(model_skipgram.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch_skipgrapm(batch_size, news_corpus)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #predict\n",
    "    loss = model_skipgram(input_tensor, label_tensor, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss:2.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram Model (Neg Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index['<UNK>'], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):  #(1, k)\n",
    "        target_index = targets[i].item()\n",
    "        nsample      = []\n",
    "        while (len(nsample) < k):\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "        \n",
    "    return torch.cat(neg_samples) #batch_size, k\n",
    "\n",
    "batch_size = 2\n",
    "x_skipgram_neg, y_skipgram_neg = random_batch_skipgrapm(batch_size, news_corpus,2)\n",
    "x_tensor = torch.LongTensor(x_skipgram_neg)\n",
    "y_tensor = torch.LongTensor(y_skipgram_neg)\n",
    "\n",
    "k = 5\n",
    "neg_samples = negative_sampling(y_tensor, unigram_table, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Skipgram (Neg Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    100 | Loss: 5.088034\n",
      "Epoch    200 | Loss: 0.805303\n",
      "Epoch    300 | Loss: 1.973518\n",
      "Epoch    400 | Loss: 1.082703\n",
      "Epoch    500 | Loss: 1.483647\n",
      "Epoch    600 | Loss: 2.702628\n",
      "Epoch    700 | Loss: 1.002316\n",
      "Epoch    800 | Loss: 0.735861\n",
      "Epoch    900 | Loss: 0.856802\n",
      "Epoch   1000 | Loss: 3.371357\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "model_neg = SkipgramNeg(voc_size, emb_size)\n",
    "optimizer = optim.Adam(model_neg.parameters(), lr=0.001)\n",
    "batch_size = 2\n",
    "emb_size   = 2\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch_skipgrapm(batch_size, news_corpus)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #predict\n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k)\n",
    "    loss = model_neg(x_tensor, y_tensor, neg_samples)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss:2.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        center_embeds  = self.center_embedding(center) #(batch_size, 1, emb_size)\n",
    "        outside_embeds = self.outside_embedding(outside) #(batch_size, 1, emb_size)\n",
    "        \n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n",
    "        \n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.LongTensor(x_glove)\n",
    "y_tensor = torch.LongTensor(y_glove)\n",
    "cooc_tensor = torch.FloatTensor(cooc)\n",
    "weighting_tensor = torch.FloatTensor(weighting)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 10 # mini-batch size\n",
    "embedding_size = 2 #so we can later plot\n",
    "voc_size = len(vocabs)\n",
    "model_glove          = Glove(voc_size, embedding_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_glove.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 55.958790 | time: 0m 0s\n",
      "Epoch: 200 | cost: 31.641260 | time: 0m 0s\n",
      "Epoch: 300 | cost: 31.378723 | time: 0m 0s\n",
      "Epoch: 400 | cost: 59.562737 | time: 0m 0s\n",
      "Epoch: 500 | cost: 15.261617 | time: 0m 0s\n",
      "Epoch: 600 | cost: 37.775932 | time: 0m 0s\n",
      "Epoch: 700 | cost: 44.425762 | time: 0m 0s\n",
      "Epoch: 800 | cost: 9.504477 | time: 0m 0s\n",
      "Epoch: 900 | cost: 79.401909 | time: 0m 0s\n",
      "Epoch: 1000 | cost: 4.851114 | time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch_glove(batch_size, news_corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)        #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model_glove(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "# glove_file = datapath('glove.6B.100d.txt')  #search on the google\n",
    "glove_file = 'glove.6B.100d.txt'  #se\n",
    "model_gensim = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_skipgram(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "        word = torch.LongTensor([word2index[word]])\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "        word = torch.LongTensor([word2index['<UNK>']])\n",
    "        \n",
    "    \n",
    "    embed_c = model_skipgram.embedding_center(word)\n",
    "    embed_o = model_skipgram.embedding_outside(word)\n",
    "    embed   = (embed_c + embed_o) / 2\n",
    "    \n",
    "    return embed[0][0].item(), embed[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_skipgram_neg(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "        word = torch.LongTensor([word2index[word]])\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "        word = torch.LongTensor([word2index['<UNK>']])\n",
    "        \n",
    "    \n",
    "    embed_c = model_neg.embedding_center(word)\n",
    "    embed_o = model_neg.embedding_outside(word)\n",
    "    embed   = (embed_c + embed_o) / 2\n",
    "    \n",
    "    return embed[0][0].item(), embed[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's write a function to get embedding given a word\n",
    "def get_embed_glove(word):\n",
    "    try:\n",
    "        id_tensor = torch.LongTensor([word2index[word]])\n",
    "    except:\n",
    "        id_tensor = torch.LongTensor([word2index['<UNK>']])\n",
    "\n",
    "    v_embed = model_glove.center_embedding(id_tensor)\n",
    "    u_embed = model_glove.outside_embedding(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more formally is to divide by its norm\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic & Syntactic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accurancy is 0.1976284584980237\n",
      "Syntactic Accurancy is  0.0641025641025641\n"
     ]
    }
   ],
   "source": [
    "## Skipgram Semantic & Syntactic Evaluation\n",
    "\n",
    "f = open(\"word_test_semantic.txt\", \"r\")\n",
    "count_all = 0\n",
    "count_correct = 0\n",
    "\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c, d = words\n",
    "    try:\n",
    "        count_all+=1\n",
    "        a_emb = np.array(get_embed_skipgram(a)).flatten()\n",
    "        b_emb = np.array(get_embed_skipgram(b)).flatten()\n",
    "        c_emb = np.array(get_embed_skipgram(c)).flatten()\n",
    "        temp = np.subtract(b_emb,np.add(a_emb,c_emb)).flatten()\n",
    "        max = -1 \n",
    "        similar_word = ''\n",
    "        for word in vocabs:\n",
    "            word_temp = get_embed_skipgram(word)\n",
    "            similar_value = cosine_similarity(temp,np.array(word_temp))\n",
    "            if similar_value > max:\n",
    "                max = similar_value\n",
    "                similar_word = word\n",
    "        if similar_word == d:\n",
    "            count_correct+=1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Semantic Accurancy is',(count_correct/count_all) * 100)\n",
    "\n",
    "f = open(\"word_test_syntactic.txt\", \"r\")\n",
    "count_all = 0\n",
    "count_correct = 0\n",
    "\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c, d = words\n",
    "    try:\n",
    "        count_all+=1\n",
    "        a_emb = np.array(get_embed_skipgram(a)).flatten()\n",
    "        b_emb = np.array(get_embed_skipgram(b)).flatten()\n",
    "        c_emb = np.array(get_embed_skipgram(c)).flatten()\n",
    "        temp = np.subtract(b_emb,np.add(a_emb,c_emb)).flatten()\n",
    "        max = -1 \n",
    "        similar_word = ''\n",
    "        for word in vocabs:\n",
    "            word_temp = get_embed_skipgram(word)\n",
    "            similar_value = cosine_similarity(temp,np.array(word_temp))\n",
    "            if similar_value > max:\n",
    "                max = similar_value\n",
    "                similar_word = word\n",
    "        if similar_word == d:\n",
    "            count_correct+=1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Syntactic Accurancy is ',(count_correct/count_all) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accurancy is 0.0\n",
      "Syntactic Accurancy is  0.0\n"
     ]
    }
   ],
   "source": [
    "## Skipgram (Neg Sampling) Semantic & Syntactic Evaluation\n",
    "\n",
    "f = open(\"word_test_semantic.txt\", \"r\")\n",
    "count_all = 0\n",
    "count_correct = 0\n",
    "\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c, d = words\n",
    "    try:\n",
    "        count_all+=1\n",
    "        a_emb = np.array(get_embed_skipgram_neg(a)).flatten()\n",
    "        b_emb = np.array(get_embed_skipgram_neg(b)).flatten()\n",
    "        c_emb = np.array(get_embed_skipgram_neg(c)).flatten()\n",
    "        temp = np.subtract(b_emb,np.add(a_emb,c_emb)).flatten()\n",
    "        max = -1 \n",
    "        similar_word = ''\n",
    "        for word in vocabs:\n",
    "            word_temp = get_embed_skipgram_neg(word)\n",
    "            similar_value = cosine_similarity(temp,np.array(word_temp))\n",
    "            if similar_value > max:\n",
    "                max = similar_value\n",
    "                similar_word = word\n",
    "        if similar_word == d:\n",
    "            count_correct+=1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Semantic Accurancy is',(count_correct/count_all) * 100)\n",
    "\n",
    "f = open(\"word_test_syntactic.txt\", \"r\")\n",
    "count_all = 0\n",
    "count_correct = 0\n",
    "\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c, d = words\n",
    "    try:\n",
    "        count_all+=1\n",
    "        a_emb = np.array(get_embed_skipgram_neg(a)).flatten()\n",
    "        b_emb = np.array(get_embed_skipgram_neg(b)).flatten()\n",
    "        c_emb = np.array(get_embed_skipgram_neg(c)).flatten()\n",
    "        temp = np.subtract(b_emb,np.add(a_emb,c_emb)).flatten()\n",
    "        max = -1 \n",
    "        similar_word = ''\n",
    "        for word in vocabs:\n",
    "            word_temp = get_embed_skipgram_neg(word)\n",
    "            similar_value = cosine_similarity(temp,np.array(word_temp))\n",
    "            if similar_value > max:\n",
    "                max = similar_value\n",
    "                similar_word = word\n",
    "        if similar_word == d:\n",
    "            count_correct+=1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Syntactic Accurancy is ',(count_correct/count_all) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accurancy is 0.0\n",
      "Syntactic Accurancy is  0.0\n"
     ]
    }
   ],
   "source": [
    "## GloVe Semantic & Syntactic Evaluation\n",
    "\n",
    "f = open(\"word_test_semantic.txt\", \"r\")\n",
    "count_all = 0\n",
    "count_correct = 0\n",
    "\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c, d = words\n",
    "    try:\n",
    "        count_all+=1\n",
    "        a_emb = np.array(get_embed_glove(a)).flatten()\n",
    "        b_emb = np.array(get_embed_glove(b)).flatten()\n",
    "        c_emb = np.array(get_embed_glove(c)).flatten()\n",
    "        temp = np.subtract(b_emb,np.add(a_emb,c_emb)).flatten()\n",
    "        max = -1 \n",
    "        similar_word = ''\n",
    "        for word in vocabs:\n",
    "            word_temp = get_embed_glove(word)\n",
    "            similar_value = cosine_similarity(temp,np.array(word_temp))\n",
    "            if similar_value > max:\n",
    "                max = similar_value\n",
    "                similar_word = word\n",
    "        if similar_word == d:\n",
    "            count_correct+=1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Semantic Accurancy is',(count_correct/count_all) * 100)\n",
    "\n",
    "f = open(\"word_test_syntactic.txt\", \"r\")\n",
    "count_all = 0\n",
    "count_correct = 0\n",
    "\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c, d = words\n",
    "\n",
    "    try:\n",
    "        count_all+=1\n",
    "        a_emb = np.array(get_embed_glove(a)).flatten()\n",
    "        b_emb = np.array(get_embed_glove(b)).flatten()\n",
    "        c_emb = np.array(get_embed_glove(c)).flatten()\n",
    "        temp = np.subtract(b_emb,np.add(a_emb,c_emb)).flatten()\n",
    "        max = -1 \n",
    "        similar_word = ''\n",
    "        for word in vocabs:\n",
    "            word_temp = get_embed_glove(word)\n",
    "            similar_value = cosine_similarity(temp,np.array(word_temp))\n",
    "            if similar_value > max:\n",
    "                max = similar_value\n",
    "                similar_word = word\n",
    "        if similar_word == d:\n",
    "            count_correct+=1\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Syntactic Accurancy is ',(count_correct/count_all) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Accurancy is 0.0\n",
      "Syntactic Accurancy is  2.307692307692308\n"
     ]
    }
   ],
   "source": [
    "## GloVe Gensim Semantic & Syntactic Evaluation\n",
    "\n",
    "f = open(\"word_test_semantic.txt\", \"r\")\n",
    "count_all = 0\n",
    "count_correct = 0\n",
    "\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c, d = words\n",
    "    count_all+=1\n",
    "    try:\n",
    "        result = model_gensim.most_similar(positive=[a, c], negative=[b]) \n",
    "        if result[0][0] == d:\n",
    "            count_correct+=1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Semantic Accurancy is',(count_correct/count_all) * 100)\n",
    "\n",
    "f = open(\"word_test_syntactic.txt\", \"r\")\n",
    "count_all = 0\n",
    "count_correct = 0\n",
    "\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c, d = words\n",
    "    count_all+=1\n",
    "    try:\n",
    "        result = model_gensim.most_similar(positive=[a, c], negative=[b]) \n",
    "        if result[0][0] == d:\n",
    "            count_correct+=1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Syntactic Accurancy is ',(count_correct/count_all) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Judgement Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"wordsim_similarity_goldstandard.txt\", \"r\")\n",
    "human_mean = []\n",
    "model_similar_skipgram = []\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c = words\n",
    "    try:\n",
    "        model_similar_skipgram.append(cosine_similarity(np.array(get_embed_skipgram(a)),np.array(get_embed_skipgram(b))))\n",
    "        human_mean.append(c)\n",
    "    except:\n",
    "        continue\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"wordsim_similarity_goldstandard.txt\", \"r\")\n",
    "model_similar_skipgram_neg = []\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c = words\n",
    "    try:\n",
    "        model_similar_skipgram_neg.append(cosine_similarity(np.array(get_embed_skipgram_neg(a)),np.array(get_embed_skipgram_neg(b))))\n",
    "    except:\n",
    "        continue\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"wordsim_similarity_goldstandard.txt\", \"r\")\n",
    "human_mean = []\n",
    "model_similar_glove = []\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c = words\n",
    "    try:\n",
    "        model_similar_glove.append(cosine_similarity(np.array(get_embed_glove(a)),np.array(get_embed_glove(b))))\n",
    "        human_mean.append(c)\n",
    "    except:\n",
    "        continue\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"wordsim_similarity_goldstandard.txt\", \"r\")\n",
    "human_mean = []\n",
    "model_similar_glove_gensim = []\n",
    "for line in f:\n",
    "    words = line.strip().split()\n",
    "    a, b, c = words\n",
    "    try:\n",
    "        model_similar_glove_gensim.append(model_gensim.distance(a,b))\n",
    "        human_mean.append(c)\n",
    "    except:\n",
    "        continue\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021424759410375093"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "res = spearmanr(human_mean, model_similar_skipgram)\n",
    "res.statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10198918072519858"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "res = spearmanr(human_mean, model_similar_skipgram_neg)\n",
    "res.statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10187570060495589"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "res = spearmanr(human_mean, model_similar_glove)\n",
    "res.statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.57701049257972"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "res = spearmanr(human_mean, model_similar_glove_gensim)\n",
    "res.statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "res = spearmanr(human_mean, human_mean)\n",
    "res.statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model          |   Window Size | Training Loss   | Training time   |   Syntactic Accuracy |   Semantic accuracy |\n",
      "|----------------+---------------+-----------------+-----------------+----------------------+---------------------|\n",
      "| Skipgram       |             4 | 10.606616       | 6 min 44 sec    |              0.19762 |             0.0641  |\n",
      "| Skipgram (NEG) |             4 | 3.371357        | 5 min 35 sec    |              0       |             0       |\n",
      "| GloVe          |             4 | 4.851114        | 1 min 37 sec    |              0       |             0       |\n",
      "| GloVe (Gensim) |             4 | -               | -               |              0       |             2.30769 |\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([['Skipgram', 4, 10.606616,'6 min 44 sec',0.19762,0.06410], \n",
    "                ['Skipgram (NEG)', 4,3.371357,'5 min 35 sec',0.0,0.0],\n",
    "                ['GloVe', 4,4.851114,'1 min 37 sec',0.0,0.0],\n",
    "                ['GloVe (Gensim)', 4,'-','-',0.0,2.30769]], \n",
    "                headers=[\"Model\",\"Window Size\",\"Training Loss\",\"Training time\",\"Syntactic Accuracy\",'Semantic accuracy'], tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model   |   Skipgram |   Skipgram (NEG) |   GloVe |   GloVe (Gensim) |   Y true |\n",
      "|---------+------------+------------------+---------+------------------+----------|\n",
      "| MSE     |     0.0214 |           0.1019 |  0.1018 |           -0.577 |        1 |\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([['MSE', 0.0214, 0.1019,0.1018,-0.5770,1]], \n",
    "                headers=['Model',\"Skipgram\",\"Skipgram (NEG)\",\"GloVe\",\"GloVe (Gensim)\",'Y true'], tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'model_skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_neg, 'model_skipgram_neg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
