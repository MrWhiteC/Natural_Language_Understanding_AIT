{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bSz5jzj61nHc"
   },
   "source": [
    "# BERT (Updated 1 Feb 2025, Available CUDA)\n",
    "\n",
    "We shall implement BERT.  For this tutorial, you may want to first look at my Transformers tutorial to get a basic understanding of Transformers. \n",
    "\n",
    "For BERT, the main difference is on how we process the datasets, i.e., masking.   Aside from that, the backbone model is still the Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-8kZmr4ItGUj"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from   random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Set GPU device\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "For simplicity, we shall use very simple data like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/standalonemac/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/standalonemac/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 21197\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load BookCorpus dataset\n",
    "# The first 1% of `train` split.\n",
    "dataset = load_dataset('roneneldan/TinyStories', split='train[:1%]')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = dataset['text']\n",
    "text = [x.lower() for x in sentences] #lower case\n",
    "text = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text] #clean all symbols\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one day a little girl named lily found a needle in her room she knew it was difficult to play with it because it was sharp lily wanted to share the needle with her mom so she could sew a button on her shirt\n",
      "\n",
      "lily went to her mom and said \"mom i found this needle can you share it with me and sew my shirt\" her mom smiled and said \"yes lily we can share the needle and fix your shirt\"\n",
      "\n",
      "together they shared the needle and sewed the button on lily's shirt it was not difficult for them because they were sharing and helping each other after they finished lily thanked her mom for sharing the needle and fixing her shirt they both felt happy because they had shared and worked together _____\n",
      "['one', 'day', 'a', 'little', 'girl', 'named', 'lily', 'found', 'a', 'needle', 'in', 'her', 'room', 'she', 'knew', 'it', 'was', 'difficult', 'to', 'play', 'with', 'it', 'because', 'it', 'was', 'sharp', 'lily', 'wanted', 'to', 'share', 'the', 'needle', 'with', 'her', 'mom', 'so', 'she', 'could', 'sew', 'a', 'button', 'on', 'her', 'shirt', 'lily', 'went', 'to', 'her', 'mom', 'and', 'said', '\"mom', 'i', 'found', 'this', 'needle', 'can', 'you', 'share', 'it', 'with', 'me', 'and', 'sew', 'my', 'shirt\"', 'her', 'mom', 'smiled', 'and', 'said', '\"yes', 'lily', 'we', 'can', 'share', 'the', 'needle', 'and', 'fix', 'your', 'shirt\"', 'together', 'they', 'shared', 'the', 'needle', 'and', 'sewed', 'the', 'button', 'on', \"lily's\", 'shirt', 'it', 'was', 'not', 'difficult', 'for', 'them', 'because', 'they', 'were', 'sharing', 'and', 'helping', 'each', 'other', 'after', 'they', 'finished', 'lily', 'thanked', 'her', 'mom', 'for', 'sharing', 'the', 'needle', 'and', 'fixing', 'her', 'shirt', 'they', 'both', 'felt', 'happy', 'because', 'they', 'had', 'shared', 'and', 'worked', 'together']\n"
     ]
    }
   ],
   "source": [
    "for sentence in text:\n",
    "    print(sentence, \"_____\")\n",
    "    words = sentence.split()\n",
    "    print(words)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making vocabs\n",
    "\n",
    "Before making the vocabs, let's remove all question marks and perios, etc, then turn everything to lowercase, and then simply split the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating word2id: 18705it [00:00, 3613747.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18709"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Combine everything into one to make vocab\n",
    "word_list = list(set(\" \".join(text).split()))\n",
    "word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}  # special tokens\n",
    "\n",
    "# Create the word2id in a single pass\n",
    "for i, w in tqdm(enumerate(word_list), desc=\"Creating word2id\"):\n",
    "    word2id[w] = i + 4  # because 0-3 are already occupied\n",
    "\n",
    "# Precompute the id2word mapping (this can be done once after word2id is fully populated)\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 21197/21197 [00:00<00:00, 71709.85it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2id)\n",
    "\n",
    "# List of all tokens for the whole text\n",
    "token_list = []\n",
    "\n",
    "# Process sentences more efficiently\n",
    "for sentence in tqdm(text, desc=\"Processing sentences\"):\n",
    "    token_list.append([word2id[word] for word in sentence.split()])\n",
    "\n",
    "# Now token_list contains the tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.',\n",
       " 'Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\\n\\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look at sentences\n",
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ42SFLKtsv_",
    "outputId": "16c28ac8-8349-48ab-f1d3-a9431e658349",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12666,\n",
       "  2860,\n",
       "  4797,\n",
       "  17342,\n",
       "  8928,\n",
       "  2184,\n",
       "  8194,\n",
       "  12918,\n",
       "  4797,\n",
       "  4599,\n",
       "  2497,\n",
       "  10823,\n",
       "  18316,\n",
       "  17852,\n",
       "  5404,\n",
       "  6076,\n",
       "  6098,\n",
       "  5333,\n",
       "  4594,\n",
       "  10073,\n",
       "  16853,\n",
       "  6076,\n",
       "  2984,\n",
       "  6076,\n",
       "  6098,\n",
       "  11954,\n",
       "  8194,\n",
       "  15775,\n",
       "  4594,\n",
       "  3620,\n",
       "  9234,\n",
       "  4599,\n",
       "  16853,\n",
       "  10823,\n",
       "  9939,\n",
       "  8135,\n",
       "  17852,\n",
       "  14186,\n",
       "  6612,\n",
       "  4797,\n",
       "  3946,\n",
       "  14822,\n",
       "  10823,\n",
       "  2812,\n",
       "  8194,\n",
       "  1522,\n",
       "  4594,\n",
       "  10823,\n",
       "  9939,\n",
       "  14987,\n",
       "  14077,\n",
       "  5063,\n",
       "  17575,\n",
       "  12918,\n",
       "  1302,\n",
       "  4599,\n",
       "  9395,\n",
       "  11967,\n",
       "  3620,\n",
       "  6076,\n",
       "  16853,\n",
       "  13387,\n",
       "  14987,\n",
       "  6612,\n",
       "  9811,\n",
       "  259,\n",
       "  10823,\n",
       "  9939,\n",
       "  15122,\n",
       "  14987,\n",
       "  14077,\n",
       "  4254,\n",
       "  8194,\n",
       "  614,\n",
       "  9395,\n",
       "  3620,\n",
       "  9234,\n",
       "  4599,\n",
       "  14987,\n",
       "  12515,\n",
       "  3572,\n",
       "  259,\n",
       "  13363,\n",
       "  13640,\n",
       "  6129,\n",
       "  9234,\n",
       "  4599,\n",
       "  14987,\n",
       "  4036,\n",
       "  9234,\n",
       "  3946,\n",
       "  14822,\n",
       "  13123,\n",
       "  2812,\n",
       "  6076,\n",
       "  6098,\n",
       "  8868,\n",
       "  5333,\n",
       "  15326,\n",
       "  6383,\n",
       "  2984,\n",
       "  13640,\n",
       "  9025,\n",
       "  2842,\n",
       "  14987,\n",
       "  12153,\n",
       "  2805,\n",
       "  14855,\n",
       "  15093,\n",
       "  13640,\n",
       "  13833,\n",
       "  8194,\n",
       "  7224,\n",
       "  10823,\n",
       "  9939,\n",
       "  15326,\n",
       "  2842,\n",
       "  9234,\n",
       "  4599,\n",
       "  14987,\n",
       "  14576,\n",
       "  10823,\n",
       "  2812,\n",
       "  13640,\n",
       "  84,\n",
       "  8316,\n",
       "  16405,\n",
       "  2984,\n",
       "  13640,\n",
       "  11674,\n",
       "  6129,\n",
       "  14987,\n",
       "  1169,\n",
       "  13363],\n",
       " [6241,\n",
       "  3924,\n",
       "  4797,\n",
       "  16332,\n",
       "  9501,\n",
       "  6098,\n",
       "  4797,\n",
       "  17342,\n",
       "  9659,\n",
       "  2184,\n",
       "  18154,\n",
       "  18154,\n",
       "  16059,\n",
       "  4594,\n",
       "  9479,\n",
       "  7763,\n",
       "  14987,\n",
       "  10073,\n",
       "  2497,\n",
       "  9234,\n",
       "  16904,\n",
       "  18154,\n",
       "  6098,\n",
       "  4797,\n",
       "  17719,\n",
       "  9659,\n",
       "  2984,\n",
       "  8380,\n",
       "  6861,\n",
       "  11674,\n",
       "  12642,\n",
       "  14412,\n",
       "  12642,\n",
       "  14412,\n",
       "  14164,\n",
       "  18154,\n",
       "  16405,\n",
       "  14987,\n",
       "  6394,\n",
       "  12666,\n",
       "  2860,\n",
       "  18154,\n",
       "  6098,\n",
       "  178,\n",
       "  2497,\n",
       "  9234,\n",
       "  12139,\n",
       "  4384,\n",
       "  8380,\n",
       "  15756,\n",
       "  4797,\n",
       "  3690,\n",
       "  14410,\n",
       "  9234,\n",
       "  14410,\n",
       "  11674,\n",
       "  12998,\n",
       "  3736,\n",
       "  944,\n",
       "  9025,\n",
       "  5429,\n",
       "  18154,\n",
       "  17676,\n",
       "  1745,\n",
       "  9234,\n",
       "  3736,\n",
       "  4213,\n",
       "  14987,\n",
       "  15775,\n",
       "  4594,\n",
       "  10073,\n",
       "  16853,\n",
       "  6383,\n",
       "  18154,\n",
       "  15398,\n",
       "  14100,\n",
       "  9234,\n",
       "  14410,\n",
       "  14987,\n",
       "  13690,\n",
       "  9234,\n",
       "  3736,\n",
       "  4213,\n",
       "  14822,\n",
       "  8451,\n",
       "  8380,\n",
       "  9217,\n",
       "  14987,\n",
       "  12818,\n",
       "  469,\n",
       "  2927,\n",
       "  18154,\n",
       "  8966,\n",
       "  16853,\n",
       "  9234,\n",
       "  5429,\n",
       "  3736,\n",
       "  4778,\n",
       "  2860,\n",
       "  4384,\n",
       "  6076,\n",
       "  6098,\n",
       "  16332,\n",
       "  4594,\n",
       "  9479,\n",
       "  2890,\n",
       "  18154,\n",
       "  5404,\n",
       "  8380,\n",
       "  1218,\n",
       "  16755,\n",
       "  14412,\n",
       "  8380,\n",
       "  1522,\n",
       "  4594,\n",
       "  9234,\n",
       "  14412,\n",
       "  6142,\n",
       "  14987,\n",
       "  7630,\n",
       "  16755,\n",
       "  17719,\n",
       "  14412,\n",
       "  13991,\n",
       "  18154,\n",
       "  6098,\n",
       "  8289,\n",
       "  4594,\n",
       "  9479,\n",
       "  7763,\n",
       "  14987,\n",
       "  10073,\n",
       "  5152,\n",
       "  9234,\n",
       "  14362,\n",
       "  2860,\n",
       "  14987,\n",
       "  18154,\n",
       "  5965,\n",
       "  1110,\n",
       "  4810,\n",
       "  15093]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look at token_list\n",
    "token_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "day\n",
      "a\n",
      "little\n",
      "girl\n",
      "named\n",
      "lily\n",
      "found\n",
      "a\n",
      "needle\n",
      "in\n",
      "her\n",
      "room\n",
      "she\n",
      "knew\n",
      "it\n",
      "was\n",
      "difficult\n",
      "to\n",
      "play\n",
      "with\n",
      "it\n",
      "because\n",
      "it\n",
      "was\n",
      "sharp\n",
      "lily\n",
      "wanted\n",
      "to\n",
      "share\n",
      "the\n",
      "needle\n",
      "with\n",
      "her\n",
      "mom\n",
      "so\n",
      "she\n",
      "could\n",
      "sew\n",
      "a\n",
      "button\n",
      "on\n",
      "her\n",
      "shirt\n",
      "lily\n",
      "went\n",
      "to\n",
      "her\n",
      "mom\n",
      "and\n",
      "said\n",
      "\"mom\n",
      "i\n",
      "found\n",
      "this\n",
      "needle\n",
      "can\n",
      "you\n",
      "share\n",
      "it\n",
      "with\n",
      "me\n",
      "and\n",
      "sew\n",
      "my\n",
      "shirt\"\n",
      "her\n",
      "mom\n",
      "smiled\n",
      "and\n",
      "said\n",
      "\"yes\n",
      "lily\n",
      "we\n",
      "can\n",
      "share\n",
      "the\n",
      "needle\n",
      "and\n",
      "fix\n",
      "your\n",
      "shirt\"\n",
      "together\n",
      "they\n",
      "shared\n",
      "the\n",
      "needle\n",
      "and\n",
      "sewed\n",
      "the\n",
      "button\n",
      "on\n",
      "lily's\n",
      "shirt\n",
      "it\n",
      "was\n",
      "not\n",
      "difficult\n",
      "for\n",
      "them\n",
      "because\n",
      "they\n",
      "were\n",
      "sharing\n",
      "and\n",
      "helping\n",
      "each\n",
      "other\n",
      "after\n",
      "they\n",
      "finished\n",
      "lily\n",
      "thanked\n",
      "her\n",
      "mom\n",
      "for\n",
      "sharing\n",
      "the\n",
      "needle\n",
      "and\n",
      "fixing\n",
      "her\n",
      "shirt\n",
      "they\n",
      "both\n",
      "felt\n",
      "happy\n",
      "because\n",
      "they\n",
      "had\n",
      "shared\n",
      "and\n",
      "worked\n",
      "together\n"
     ]
    }
   ],
   "source": [
    "#testing one sentence\n",
    "for tokens in token_list[0]:\n",
    "    print(id2word[tokens])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loader\n",
    "\n",
    "We gonna make dataloader.  Inside here, we need to make two types of embeddings: **token embedding** and **segment embedding**\n",
    "\n",
    "1. **Token embedding** - Given “The cat is walking. The dog is barking”, we add [CLS] and [SEP] >> “[CLS] the cat is walking [SEP] the dog is barking”. \n",
    "\n",
    "2. **Segment embedding**\n",
    "A segment embedding separates two sentences, i.e., [0 0 0 0 1 1 1 1 ]\n",
    "\n",
    "3. **Masking**\n",
    "As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. In this 15%, 80% is replaced with masks, while 10% is replaced with random tokens, and the rest 10% is left as is.  Here we specified `max_pred` \n",
    "\n",
    "4. **Padding**\n",
    "Once we mask, we will add padding. For simplicity, here we padded until some specified `max_len`. \n",
    "\n",
    "Note:  `positive` and `negative` are just simply counts to keep track of the batch size.  `positive` refers to two sentences that are really next to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_mask   = 5  # max masked tokens when 15% exceed, it will only be max_pred\n",
    "max_len    = 700 # maximum of length to be padded; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TtyOOmRntu8w"
   },
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0  #count of batch size;  we want to have half batch that are positive pairs (i.e., next sentence pairs)\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        \n",
    "        #randomly choose two sentence so we can put [SEP]\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        #retrieve the two sentences\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        #1. token embedding - append CLS and SEP\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "\n",
    "        #2. segment embedding - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        #3. mask language modeling\n",
    "        #masked 15%, but should be at least 1 but does not exceed max_mask\n",
    "        n_pred =  min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get the pos that excludes CLS and SEP and shuffle them\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] and token != word2id['[SEP]']]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        #simply loop and change the input_ids to [MASK]\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)  #remember the position\n",
    "            masked_tokens.append(input_ids[pos]) #remember the tokens\n",
    "            #80% replace with a [MASK], but 10% will replace with a random token\n",
    "            if random() < 0.1:  # 10%\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = word2id[id2word[index]] # replace\n",
    "            elif random() < 0.9:  # 80%\n",
    "                input_ids[pos] = word2id['[MASK]'] # make mask\n",
    "            else:  #10% do nothing\n",
    "                pass\n",
    "\n",
    "        # pad the input_ids and segment ids until the max len\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # pad the masked_tokens and masked_pos to make sure the lenth is max_mask\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        #check if first sentence is really comes before the second sentence\n",
    "        #also make sure positive is exactly half the batch size\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "            \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Q7_HC-Y0jC3K"
   },
   "outputs": [],
   "source": [
    "batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len of batch\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 700]),\n",
       " torch.Size([6, 700]),\n",
       " torch.Size([6, 5]),\n",
       " torch.Size([6, 5]),\n",
       " torch.Size([6]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can deconstruct using map and zip\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "Recall that BERT only uses the encoder.\n",
    "\n",
    "BERT has the following components:\n",
    "\n",
    "- Embedding layers\n",
    "- Attention Mask\n",
    "- Encoder layer\n",
    "- Multi-head attention\n",
    "- Scaled dot product attention\n",
    "- Position-wise feed-forward network\n",
    "- BERT (assembling all the components)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Embedding\n",
    "\n",
    "Here we simply generate the positional embedding, and sum the token embedding, positional embedding, and segment embedding together.\n",
    "\n",
    "<img src = \"figures/BERT_embed.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_segments, d_model, device):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        #x, seg: (bs, len)\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long).to(self.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "s1PGksqBNuZM"
   },
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k, device):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1).to(device)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 700, 700])\n"
     ]
    }
   ],
   "source": [
    "print(get_attn_pad_mask(input_ids, input_ids, device).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Encoder\n",
    "\n",
    "The encoder has two main components: \n",
    "\n",
    "- Multi-head Attention\n",
    "- Position-wise feed-forward network\n",
    "\n",
    "First let's make the wrapper called `EncoderLayer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k, device)\n",
    "        self.pos_ffn       = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the scaled dot attention, to be used inside the multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, device):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_k])).to(device)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / self.scale # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6    # number of Encoder of Encoder Layer\n",
    "n_heads  = 8    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Multiheadattention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k, device):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_k\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, self.d_v * n_heads)\n",
    "        self.device = device\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention(self.d_k, self.device)(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(self.n_heads * self.d_v, self.d_model, device=self.device)(context)\n",
    "        return nn.LayerNorm(self.d_model, device=self.device)(output + residual), attn # output: [batch_size x len_q x d_model]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the PoswiseFeedForwardNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(F.gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Putting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "OZ0TJ84W4SZw"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device):\n",
    "        super(BERT, self).__init__()\n",
    "        self.params = {'n_layers': n_layers, 'n_heads': n_heads, 'd_model': d_model,\n",
    "                       'd_ff': d_ff, 'd_k': d_k, 'n_segments': n_segments,\n",
    "                       'vocab_size': vocab_size, 'max_len': max_len}\n",
    "        self.embedding = Embedding(vocab_size, max_len, n_segments, d_model, device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        \n",
    "        # 1. predict next sentence\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        # 2. predict the masked token\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_nsp\n",
    "    \n",
    "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_layers = 12    # number of Encoder of Encoder Layer\n",
    "n_heads  = 12    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = d_model * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2\n",
    "\n",
    "num_epoch = 1000\n",
    "model = BERT(\n",
    "    n_layers, \n",
    "    n_heads, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    d_k, \n",
    "    n_segments, \n",
    "    vocab_size, \n",
    "    max_len, \n",
    "    device\n",
    ").to(device)  # Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UAG3SEP4UbU",
    "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 loss = 110.094528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|█         | 100/1000 [01:41<15:09,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 loss = 4.995957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|██        | 200/1000 [03:23<13:31,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200 loss = 4.389479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|███       | 300/1000 [05:03<11:29,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300 loss = 4.008092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|████      | 400/1000 [06:43<09:49,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 400 loss = 4.012590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  50%|█████     | 500/1000 [08:22<08:17,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500 loss = 3.990366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  60%|██████    | 600/1000 [10:02<06:39,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600 loss = 3.966225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  70%|███████   | 700/1000 [11:41<04:59,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 700 loss = 3.967532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  80%|████████  | 800/1000 [13:21<03:19,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 800 loss = 3.967981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  90%|█████████ | 900/1000 [15:01<01:39,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 900 loss = 3.987228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 1000/1000 [16:41<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "# Move inputs to GPU\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "masked_tokens = masked_tokens.to(device)\n",
    "masked_pos = masked_pos.to(device)\n",
    "isNext = isNext.to(device)\n",
    "\n",
    "# Wrap the epoch loop with tqdm\n",
    "for epoch in tqdm(range(num_epoch), desc=\"Training Epochs\"):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
    "    #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
    "    #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
    "\n",
    "    #1. mlm loss\n",
    "    #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    #2. nsp loss\n",
    "    #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
    "    loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
    "    \n",
    "    #3. combine loss\n",
    "    loss = loss_lm + loss_nsp\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bert_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model after training\n",
    "torch.save(model.state_dict(), 'bert_model.pth')\n",
    "print(\"Model saved to bert_model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference\n",
    "\n",
    "Since our dataset is very small, it won't work very well, but just for the sake of demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uD3K8T6B4YJp",
    "outputId": "e057405b-1f78-431c-fa71-032a738fb848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'lily', 'and', 'ben', 'were', 'twins', 'who', 'liked', 'to', 'help', 'their', 'mom', 'bake', 'pies', '[MASK]', 'day', 'mom', 'made', 'a', 'big', 'apple', 'pie', 'and', 'put', 'it', 'on', 'the', 'table', 'to', 'cool', 'she', 'told', 'lily', 'and', 'ben', 'not', 'to', 'touch', 'the', 'pie', 'until', 'it', 'was', 'ready', 'but', 'lily', 'and', 'ben', 'were', 'very', 'hungry', 'and', 'curious', 'they', 'wanted', 'to', '[MASK]', 'the', '[MASK]', 'they', 'waited', 'until', 'mom', 'went', 'to', 'the', 'garden', 'and', 'then', 'they', 'tiptoed', 'to', 'the', 'table', 'lily', 'reached', 'for', 'the', 'pie', 'and', 'lifted', 'it', 'with', 'both', 'hands', 'it', 'was', 'heavy', 'and', 'hot', '\"be', 'careful', 'lily\"', 'ben', 'whispered', '\"don\\'t', 'drop', 'the', 'pie\"', 'but', 'lily', 'was', 'not', 'careful', 'she', 'lost', 'her', 'balance', 'and', 'the', 'pie', 'slipped', 'from', 'her', 'hands', 'it', 'fell', 'on', 'the', 'floor', 'with', 'a', 'loud', 'thud', 'the', 'pie', 'broke', 'into', 'pieces', 'and', 'made', 'a', 'big', 'mess', '\"oh', 'no\"', 'lily', 'cried', '\"we', 'dropped', 'the', 'pie\"', 'mom', 'heard', 'the', 'noise', 'and', 'came', 'back', 'to', 'the', 'kitchen', 'she', 'saw', 'the', 'pie', 'on', 'the', 'floor', 'and', 'lily', 'and', 'ben', 'with', 'sad', 'faces', '\"what', 'happened', 'here\"', 'mom', 'asked', '\"we\\'re', 'sorry', 'mom\"', 'lily', 'and', 'ben', 'said', '\"we', 'wanted', 'to', 'try', 'the', 'pie', 'and', 'we', 'dropped', 'it\"', 'mom', 'was', 'not', 'happy', 'but', 'she', 'was', 'not', 'angry', 'either', 'she', 'hugged', 'lily', 'and', 'ben', 'and', 'said', '\"it\\'s', 'okay', 'my', 'lively', 'twins', 'i', 'know', 'you', \"didn't\", 'mean', 'to', 'drop', 'the', 'pie', 'but', 'next', 'time', 'please', 'listen', 'to', 'me', 'and', 'wait', 'until', 'the', 'pie', 'is', 'cool', 'now', \"let's\", 'clean', 'up', 'this', 'mess', 'and', 'make', 'a', 'new', 'pie', 'together\"', '[SEP]', 'once', 'upon', 'a', 'time', 'there', 'was', 'a', 'gentle', 'hippo', 'who', 'lived', 'in', 'a', 'big', 'river', 'he', 'loved', 'to', 'swim', 'and', 'play', 'with', 'his', 'friends', 'one', 'day', '[MASK]', 'hippo', 'saw', 'a', 'butterfly', 'and', 'he', 'wanted', 'to', 'catch', 'it', 'he', 'chased', 'the', 'butterfly', 'but', 'accidentally', 'stepped', '[MASK]', 'a', 'sharp', 'rock', 'it', 'hurt', 'him', 'very', 'much', 'the', 'hippo', 'was', 'sad', 'and', 'cried', 'a', 'lot', 'his', 'friends', 'tried', 'to', 'help', 'him', 'but', 'nothing', 'worked', 'then', 'a', 'wise', 'old', 'turtle', 'came', 'and', 'said', '\"don\\'t', 'worry', 'my', 'friend', 'this', 'pain', 'will', 'change', 'you', 'you', 'will', 'become', 'stronger', 'and', 'wiser\"', 'the', 'hippo', \"didn't\", 'understand', 'what', 'the', 'turtle', 'meant', 'but', 'he', 'trusted', 'him', 'he', 'rested', 'and', 'ate', 'healthy', 'food', 'after', 'a', 'while', 'the', 'hippo', 'felt', 'better', 'than', 'ever', 'before', 'he', 'realized', 'that', 'the', 'pain', 'made', 'him', 'stronger', 'and', 'wiser', 'just', 'like', 'the', 'turtle', 'said', 'from', 'that', 'day', 'on', 'the', 'hippo', 'was', 'grateful', 'for', 'the', 'pain', 'and', 'never', 'chased', 'butterflies', 'again', '[SEP]']\n",
      "masked tokens (words) :  ['the', 'taste', 'one', 'on', 'pie']\n",
      "masked tokens list :  [9234, 16122, 12666, 14822, 5623]\n",
      "masked tokens (words) :  ['with', 'with', 'with', 'with', 'with']\n",
      "predict masked tokens list :  [16853, 16853, 16853, 16853, 16853]\n",
      "0\n",
      "isNext :  False\n",
      "predict isNext :  False\n"
     ]
    }
   ],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[2]))\n",
    "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "masked_tokens = masked_tokens.to(device)\n",
    "masked_pos = masked_pos.to(device)\n",
    "isNext = isNext.to(device)\n",
    "\n",
    "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
    "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
    "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
    "\n",
    "#predict masked tokens\n",
    "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
    "logits_lm = logits_lm.data.cpu().max(2)[1][0].data.numpy() \n",
    "#note that zero is padding we add to the masked_tokens\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n",
    "print('predict masked tokens list : ', [pos for pos in logits_lm])\n",
    "\n",
    "#predict nsp\n",
    "logits_nsp = logits_nsp.cpu().data.max(1)[1][0].data.numpy()\n",
    "print(logits_nsp)\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_nsp else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a bigger dataset should be able to see the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNLI and MNLI datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'premise': Value(dtype='string', id=None),\n",
       "  'hypothesis': Value(dtype='string', id=None),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None),\n",
       "  'idx': Value(dtype='int32', id=None)},\n",
       " {'premise': Value(dtype='string', id=None),\n",
       "  'hypothesis': Value(dtype='string', id=None),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "snli = datasets.load_dataset('snli')\n",
    "mnli = datasets.load_dataset('glue', 'mnli')\n",
    "mnli['train'].features, snli['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of datasets to remove 'idx' column from\n",
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'idx' column from each dataset\n",
    "for column_names in mnli.column_names.keys():\n",
    "    mnli[column_names] = mnli[column_names].remove_columns('idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([-1,  0,  1,  2]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are -1 values in the label feature, these are where no class could be decided so we remove\n",
    "snli = snli.filter(\n",
    "    lambda x: 0 if x['label'] == -1 else 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([0, 1, 2]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have your two DatasetDict objects named snli and mnli\n",
    "from datasets import DatasetDict\n",
    "# Merge the two DatasetDict objects\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': datasets.concatenate_datasets([snli['train'], mnli['train']]).shuffle(seed=55).select(list(range(1000))),\n",
    "    'test': datasets.concatenate_datasets([snli['test'], mnli['test_mismatched']]).shuffle(seed=55).select(list(range(100))),\n",
    "    'validation': datasets.concatenate_datasets([snli['validation'], mnli['validation_mismatched']]).shuffle(seed=55).select(list(range(1000)))\n",
    "})\n",
    "#remove .select(list(range(1000))) in order to use full dataset\n",
    "# Now, merged_dataset_dict contains the combined datasets from snli and mnli\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_premise = raw_dataset['train']['premise']\n",
    "text_premise = [x.lower() for x in sentences_premise] #lower case\n",
    "text_premise = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text_premise] #clean all symbols\n",
    "# text\n",
    "\n",
    "sentences_hypothesis = raw_dataset['train']['hypothesis']\n",
    "text_hypothesis = [x.lower() for x in sentences_hypothesis] #lower case\n",
    "text_hypothesis = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text_hypothesis] #clean all symbols\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating word2id: 3956it [00:00, 3845345.68it/s]\n",
      "Creating word2id: 2502it [00:00, 4374384.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Combine everything into one to make vocab\n",
    "word_list_premise = list(set(\" \".join(text_premise).split()))\n",
    "word_list_hypothesis = list((set(\" \".join(text_hypothesis).split())))\n",
    "\n",
    "word2id_premise = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}  # special tokens\n",
    "word2id_hypothesis = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}  # special tokens\n",
    "\n",
    "# Create the word2id in a single pass\n",
    "for i, w in tqdm(enumerate(word_list_premise), desc=\"Creating word2id\"):\n",
    "    word2id_premise[w] = i + 4  # because 0-3 are already occupied\n",
    "\n",
    "for i, w in tqdm(enumerate(word_list_hypothesis), desc=\"Creating word2id\"):\n",
    "    word2id_hypothesis[w] = i + 4  # because 0-3 are already occupied\n",
    "\n",
    "# Precompute the id2word mapping (this can be done once after word2id is fully populated)\n",
    "id2word_premise = {v: k for k, v in word2id_premise.items()}\n",
    "vocab_size_premis = len(word2id_premise)\n",
    "\n",
    "\n",
    "id2word_hypothesis = {v: k for k, v in word2id_hypothesis.items()}\n",
    "vocab_size_hypothesis = len(word2id_hypothesis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 1000/1000 [00:00<00:00, 463356.61it/s]\n",
      "Processing sentences: 100%|██████████| 1000/1000 [00:00<00:00, 846137.58it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2id)\n",
    "\n",
    "# List of all tokens for the whole text\n",
    "token_list_premise = []\n",
    "token_list_hypothesis = []\n",
    "\n",
    "# Process sentences more efficiently\n",
    "for sentence in tqdm(text_premise, desc=\"Processing sentences\"):\n",
    "    token_list_premise.append([word2id_premise[word] for word in sentence.split()])\n",
    "\n",
    "for sentence in tqdm(text_hypothesis, desc=\"Processing sentences\"):\n",
    "    token_list_hypothesis.append([word2id_hypothesis[word] for word in sentence.split()])\n",
    "\n",
    "# Now token_list contains the tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_mask   = 5  # max masked tokens when 15% exceed, it will only be max_pred\n",
    "max_len    = 700 # maximum of length to be padded; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    #randomly choose two sentence so we can put [SEP]\n",
    "    i = 0\n",
    "    while i != 6:\n",
    "        #retrieve the two sentences\n",
    "        tokens_a = token_list_premise[i]\n",
    "    \n",
    "        #1. token embedding - append CLS and SEP\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']]\n",
    "    \n",
    "        #2. segment embedding - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1)\n",
    "    \n",
    "        #3. mask language modeling\n",
    "        #masked 15%, but should be at least 1 but does not exceed max_mask\n",
    "        n_pred =  min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get the pos that excludes CLS and SEP and shuffle them\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] and token != word2id['[SEP]']]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_pos = []\n",
    "        #simply loop and change the input_ids to [MASK]\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)  #remember the position\n",
    "\n",
    "        # pad the input_ids and segment ids until the max len\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "    \n",
    "        # pad the masked_tokens and masked_pos to make sure the lenth is max_mask\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "    \n",
    "        #check if first sentence is really comes before the second sentence\n",
    "        #also make sure positive is exactly half the batch size\n",
    "        batch.append([input_ids, segment_ids, masked_pos]) # NotNext\n",
    "        i += 1 \n",
    "            \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_a = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 700]), torch.Size([6, 700]), torch.Size([6, 5]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_a, segment_ids_a, masked_pos_a = map(torch.LongTensor, zip(*batch_a))\n",
    "input_ids_a.shape, segment_ids_a.shape, masked_pos_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    #randomly choose two sentence so we can put [SEP]\n",
    "    i = 0\n",
    "    while i != 6:\n",
    "        #retrieve the two sentences\n",
    "        tokens_b = token_list_hypothesis[i]\n",
    "    \n",
    "        #1. token embedding - append CLS and SEP\n",
    "        input_ids = [word2id['[CLS]']] + tokens_b + [word2id['[SEP]']]\n",
    "    \n",
    "        #2. segment embedding - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        segment_ids = [0] * (1 + len(tokens_b) + 1)\n",
    "    \n",
    "        #3. mask language modeling\n",
    "        #masked 15%, but should be at least 1 but does not exceed max_mask\n",
    "        n_pred =  min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get the pos that excludes CLS and SEP and shuffle them\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] and token != word2id['[SEP]']]\n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_pos = []\n",
    "        #simply loop and change the input_ids to [MASK]\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)  #remember the position\n",
    "\n",
    "        # pad the input_ids and segment ids until the max len\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "    \n",
    "        # pad the masked_tokens and masked_pos to make sure the lenth is max_mask\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "    \n",
    "        #check if first sentence is really comes before the second sentence\n",
    "        #also make sure positive is exactly half the batch size\n",
    "        batch.append([input_ids, segment_ids, masked_pos]) # NotNext\n",
    "        i += 1 \n",
    "            \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_b = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 700]), torch.Size([6, 700]), torch.Size([6, 5]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_b, segment_ids_b, masked_pos_b = map(torch.LongTensor, zip(*batch_a))\n",
    "input_ids_b.shape, segment_ids_b.shape, masked_pos_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_a = input_ids_a.to(device)\n",
    "segment_ids_a = segment_ids_a.to(device)\n",
    "masked_pos_a = masked_pos_a.to(device)\n",
    "\n",
    "input_ids_b = input_ids_b.to(device)\n",
    "segment_ids_b = segment_ids_b.to(device)\n",
    "masked_pos_b = masked_pos_b.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_head = torch.nn.Linear(56127, 3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# define mean pooling function\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "label = []\n",
    "while i != 6:\n",
    "    label.append(raw_dataset['train']['label'][i])\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss = 6.826176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | loss = 9.535089\n",
      "Similarity Score is 0.666640559832255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 2\n",
    "label_iter = 0\n",
    "similarity_score = 0\n",
    "# 1 epoch should be enough, increase if wanted\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()  \n",
    "    classifier_head.train()\n",
    "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "    for step, batch in enumerate(tqdm(range(num_epoch), leave=True)):\n",
    "        # zero all gradients on each new step\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "        \n",
    "        # prepare batches and more all to the active device\n",
    "\n",
    "        label = label.to(device)\n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        u,_ = model(input_ids_a, segment_ids_a, masked_pos_a)  \n",
    "        v,_ = model(input_ids_b, segment_ids_b, masked_pos_b)  \n",
    "\n",
    "        # build the |u-v| tensor\n",
    "        uv = torch.sub(u, v)   # batch_size,hidden_dim\n",
    "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "        \n",
    "        # concatenate u, v, |u-v|\n",
    "        x = torch.cat([u, v, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "        \n",
    "        # process concatenated tensor through classifier_head\n",
    "        x = classifier_head(x) #batch_size, classifer\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # calculate the 'softmax-loss' between predicted and true label\n",
    "        loss = criterion(x, label)\n",
    "        label_iter += 1\n",
    "        \n",
    "        # using loss, calculate gradients and then optimizerize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_classifier.step()\n",
    "\n",
    "        similarity_score = similarity_score +  cosine_similarity(u.reshape(1, -1).detach().cpu(), v.reshape(1, -1).detach().cpu())[0, 0]\n",
    "\n",
    "        \n",
    "    print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f}')\n",
    "print('Similarity Score is',similarity_score/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 16.666666666666664\n"
     ]
    }
   ],
   "source": [
    "total = 0 \n",
    "accuracy = 0\n",
    "for i in range(label.size(0)):\n",
    "   total += 1\n",
    "   if torch.argmax(x,dim=1)[i].item() == label[i].item():\n",
    "      accuracy += 1\n",
    "      \n",
    "print('Accuracy is',((accuracy / total) * 100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to bert_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model after training\n",
    "torch.save(model.state_dict(), 'sentence_classification.pth')\n",
    "print(\"Model saved to bert_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality study.'\n",
    "sentence_b = \"Your contribution were of no help with our student's learn.\"\n",
    "\n",
    "text_a = sentence_a.lower()#lower case\n",
    "text_a = re.sub(\"[.,!?\\\\-]\", '', text_a) #clean all symbols\n",
    "\n",
    "text_b = sentence_b.lower()#lower case\n",
    "text_b = re.sub(\"[.,!?\\\\-]\", '', text_b) #clean all symbols\n",
    "\n",
    "\n",
    "\n",
    "token_list_a = []\n",
    "\n",
    "# Process sentences more efficiently\n",
    "\n",
    "token_list_a.append([word2id[word] for word in text_a.split()])\n",
    "\n",
    "token_list_b = []\n",
    "\n",
    "# Process sentences more efficiently\n",
    "\n",
    "token_list_b.append([word2id[word] for word in text_b.split()])\n",
    "\n",
    "\n",
    "\n",
    "input_ids_a = [word2id['[CLS]']] + token_list_a[0] + [word2id['[SEP]']]\n",
    "input_ids_b = [word2id['[CLS]']] + token_list_b[0] + [word2id['[SEP]']]\n",
    "\n",
    "segment_ids_a = [0] * (1 + len(token_list_a[0]) + 1)\n",
    "segment_ids_b = [0] * (1 + len(token_list_b[0]) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_maked_pos = [i for i, token in enumerate(input_ids_a) if token != word2id['[CLS]'] and token != word2id['[SEP]']]\n",
    "shuffle(cand_maked_pos)\n",
    "masked_pos_a = []\n",
    "for pos in cand_maked_pos[:5]:\n",
    "        masked_pos_a.append(pos)  \n",
    "\n",
    "n_pad = max_len - len(input_ids_a)\n",
    "input_ids_a.extend([0] * n_pad)\n",
    "segment_ids_a.extend([0] * n_pad)\n",
    "\n",
    "if max_mask > 5:\n",
    "        n_pad = max_mask - 5\n",
    "        masked_pos_a.extend([0] * n_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_maked_pos = [i for i, token in enumerate(segment_ids_b) if token != word2id['[CLS]'] and token != word2id['[SEP]']]\n",
    "shuffle(cand_maked_pos)\n",
    "masked_pos_b = []\n",
    "for pos in cand_maked_pos[:5]:\n",
    "        masked_pos_b.append(pos)  \n",
    "\n",
    "n_pad = max_len - len(input_ids_b)\n",
    "input_ids_b.extend([0] * n_pad)\n",
    "segment_ids_b.extend([0] * n_pad)\n",
    "\n",
    "if max_mask > 5:\n",
    "        n_pad = max_mask - 5\n",
    "        masked_pos_b.extend([0] * n_pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_a = torch.LongTensor(input_ids_a).to(device)\n",
    "segment_ids_a = torch.LongTensor(segment_ids_a).to(device)\n",
    "masked_pos_a = torch.LongTensor(masked_pos_a).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_b = torch.LongTensor(input_ids_b).to(device)\n",
    "segment_ids_b = torch.LongTensor(segment_ids_b).to(device)\n",
    "masked_pos_b = torch.LongTensor(masked_pos_b).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_a,_ = model(input_ids_a.unsqueeze(0), segment_ids_a.unsqueeze(0), masked_pos_a.unsqueeze(0))  \n",
    "result_b,_ = model(input_ids_b.unsqueeze(0), segment_ids_b.unsqueeze(0), masked_pos_b.unsqueeze(0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 11.6655,  15.0687,  38.4533,  ..., -25.6486,  28.4112, -47.6650]],\n",
       "       device='mps:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_a.reshape(1, -1)\n",
    "result_b.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_score = cosine_similarity(result_a.reshape(1, -1).detach().cpu(), result_b.reshape(1, -1).detach().cpu())[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(similarity_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
