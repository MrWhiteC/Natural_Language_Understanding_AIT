# Assignment 4 Deliver Details (st124903_Vorameth)

This assignment will guide you in training a pre-trained model like BERT from scratch, focusing on
leveraging text embeddings to capture semantic similarity. Additionally, we will explore how to adapt the
loss function for tasks like Natural Language Inference (NLI) to enhance the modelâ€™s ability to understand
semantic relationships between texts.

## Task 1: Training BERT from Scratch

In task 1, the training process will be done through the Bidirectional encoder representations from transformers (BERT) which written from scratch from **Prof. Chaklam Silpasuwanchai** ([BERT](https://github.com/chaklam-silpasuwanchai/Python-fo-Natural-Language-Processing/tree/main/Code/02%20-%20DL/04%20-%20Masked%20Language%20Model)). The detailed model is: 
- Number of Encoder of Encoder Layer is 12
- Number of heads in Multi-Head Attention is 12
- Embedding Size is 768
- FeedForward dimension is 768 * 4 
- Dimension of K, Q, and V is is 64
- Number of Segments is 2


For dataset, [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) will be used to train the model which detailed will be:
- Train : 2.12 M Rows
- Validation : 22 K Rows
- Synthetically Generated by GPT-3.5 and GPT-4
- Created on HuggingFace by roneneldan

In this assignment, due to resource limitation, only train with 1 percent (21197 Rows) will be used only for education purpose. 

```python
dataset = load_dataset('roneneldan/TinyStories', split='train[:1%]')
```

Pre-processing in this assignment will be done through following list : 
- Tokenization : Lower Case and Remove Special Characters
- Create Word to ID list
- Create ID to Word list

Moreover, the dataloader is the main focus in BERT model which could help the model to perform according to prefer layers and outputs. Here are some process that will be done for dataloader : 
- input_ids : CLS + token_a + SEP + token_b + SEP = This is done for telling the model with 2 seprate sentence. 
- segment_ids : Identify for the model which token a in sentence a and token b in setence b.
- masked_tokens and masked_pos : Masked tokens for model to predict. 

## Task 2: Sentence Embedding with Sentence BERT

To extend the application of BERT, the Sentence BERT or S-BERT had been introduced which the BERT model will be trained with The Stanford Natural Language Inference (SNLI) Corpus or The Multi-Genre Natural Language Inference (MNLI) corpus for applying classification problem into S-BERT model. Futhuremore, the model will be trained with siamese network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity.

In this assignment, both SNLI and MNLI will be used to train the model. Any preprocess and dataloader will use the coding from the pre-trained model for making sure the input data that should be the same.

```python
snli = datasets.load_dataset('snli')
mnli = datasets.load_dataset('glue', 'mnli')
sentences_premise = raw_dataset['train']['premise'] >  batch_a = make_batch()
sentences_hypothesis = raw_dataset['train']['hypothesis'] >  batch_b = make_batch()
```

Then during the training process, the siamese network structures with Classification Objective Function (Softmax Loss) will be focused. 

```python
# predict the last hidden state
u,_ = model(input_ids_a, segment_ids_a, masked_pos_a)  
v,_ = model(input_ids_b, segment_ids_b, masked_pos_b)  

# |u-v| tensor
uv = torch.sub(u, v)  
uv_abs = torch.abs(uv) 

# concatenate u, v, |u-v|
x = torch.cat([u, v, uv_abs], dim=-1) 

# process concatenated tensor through classifier_head
x = classifier_head(x) 

# reduce for input into loss funciton
x = x.mean(dim=1)

# softmax loss
loss = criterion(x, label)
```

Based on the above softmax loss output, the loss is average with (2 epoches were run due to resource limitation)

## Task 3: Evaluation and Analysis
1. Metrices
    Model Type | SNLI OR MNLI Performance
    --- | --- 
    Our Model | Loss = 9.535, Accuracy =  16.666|


2. Analysis
    - Limitations or Challenges
        - The main challenges for this assignment is applied BERT into S-BERT model. With the from scratch implmentation, the model could not completely comply with the variables and paramter on the BERT from Google model. This is enforced that the datasets have to be applied with from scrath model which quite difficult to adjust due to problems of completely different applications. BERT requires 3 paramters (input,segment, masked positiion) whick will predict masked word while S-BERT requires 2 paramters (input, attent mask) which will predict the context classification. This could cause mismatch on size or dimension. From aformentioned, adaptation from BERT with datasets to train S-BERT is challenging.

        - Training size is another problems due to the resources limitation. In puffer server, the out of memory always occur due to share resource pools. To workaround, only 2 epoches will be used to trained the S-BERT model. Moreover, 1 percent of a millions datasets are trained in BERT model. This cause model to always predict in entailment only.

    - Propose Potential Improvements or Modifications.
        - To solve translation problems, the implementors should study more on how to transfer the BERT to S-BERT model. This means that weights, parameters, dimension, layers should be correlated and correctly defined. Moreover, the dataloader should be more specific for SNLI and MNLI datasets which receive two sentence for comparing the similarity while the original datasets sample 2 sentences to predict the masked tokens. Based on this, the dataloader should be coded to make the SNLI and MNLI datasets to comply with original dataset manner and including adjustment the paramter for BERT model parameters, outputs, and layers to accept specific application for classification.

        - To improve the accuracy, the training size should be increased which model could capture variety of the word from the shuffle method in the dataloader. Moreover, the number epoches should be increased in order to let the model learn in deeper weight adjusting which could help to capture more relation in the masked words. In addition, to prevent dominant prediction, the regularization should be applied together with more sampled data. 



## Task 4: Web Application Development

The webiste is built based on S-BERT model which it will predict the relationships between 2 sentences whether they are entailment, contradiction, and neutral. A breif tech stacks are Flask as the backend and HTML as the frontend.

Input A : Text A
Input B : Text B

Result : Classification of Both Text

Example 1

![website](https://github.com/MrWhiteC/Natural_Language_Understanding_AIT/blob/main/Assignment4/images/website1.png)

Example 2

![website](https://github.com/MrWhiteC/Natural_Language_Understanding_AIT/blob/main/Assignment4/images/website2.png)

Example 3

![website](https://github.com/MrWhiteC/Natural_Language_Understanding_AIT/blob/main/Assignment4/images/website3.png)

    